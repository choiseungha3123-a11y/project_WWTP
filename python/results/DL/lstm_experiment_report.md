# LSTM 모델 실험 결과 보고서

## 📊 실험 개요

**목적**: 하수처리장 유입 유량(y_Q_in) 예측을 위한 LSTM 모델 최적화  
**데이터**: modelFLOW_dataset.csv (26,193개 샘플, 203개 특성)  
**타겟 변수**: y_Q_in = y_flowA + y_flowB  
**데이터 분할**: Train 60% / Validation 20% / Test 20%  
**시퀀스 길이**: 24 시간 스텝

---

## 🔬 실험 결과 요약

### 실험 1: 기본 LSTM 모델 (Baseline)

**모델 구조**:
- LSTM 레이어: 2층
- 은닉층 크기: 64 유닛
- 드롭아웃: 0.2
- 출력층: 단일 FC 레이어
- 총 파라미터: 102,209개

**학습 설정**:
- 배치 크기: 32
- 학습률: 0.001
- 최대 에포크: 100
- 조기 종료 patience: 10

**성능 결과**:
| 데이터셋 | Loss | RMSE | MAE | R² |
|---------|------|------|-----|-----|
| Train | 0.1849 | 36.92 | 22.13 | **0.8154** |
| Validation | 0.3529 | 51.02 | 29.53 | **0.5977** |
| Test | 0.4782 | 59.39 | 39.50 | **0.4032** |

**특징**:
- ✅ 가장 빠른 수렴 (18 에포크)
- ✅ 상대적으로 안정적인 학습
- ⚠️ Train-Test 성능 격차 큼 (R²: 0.8154 → 0.4032)
- ⚠️ 일반화 성능 부족

---

### 실험 2: 깊은 LSTM 모델 (Deep Network)

**모델 구조**:
- LSTM 레이어: **4층** ⬆️
- 은닉층 크기: **128 유닛** ⬆️
- 드롭아웃: 0.3
- 출력층: 단일 FC 레이어
- 총 파라미터: **566,913개** (5.5배 증가)

**학습 설정**:
- 배치 크기: 32
- 학습률: 0.0005 ⬇️
- 최대 에포크: 150
- 조기 종료 patience: 15

**성능 결과**:
| 데이터셋 | Loss | RMSE | MAE | R² |
|---------|------|------|-----|-----|
| Train | 0.1425 | 32.42 | 20.60 | **0.8577** |
| Validation | 0.4148 | 55.32 | 33.44 | **0.5271** |
| Test | 0.7652 | 75.13 | 50.45 | **0.0450** |

**특징**:
- ✅ Train 성능 향상 (R²: 0.8577)
- ❌ **심각한 과적합 발생**
- ❌ Test R² 급격히 하락 (0.4032 → 0.0450)
- ❌ Train-Test 격차 최대 (R²: 0.8577 → 0.0450)
- ⚠️ 모델 복잡도 과다

---

### 실험 3: 정규화 강화 모델 (Regularized Network) ⭐

**모델 구조**:
- LSTM 레이어: **3층** (4층에서 감소)
- 은닉층 크기: 128 유닛
- 드롭아웃: **0.4** ⬆️
- **배치 정규화 추가** 🆕
- 출력층: **2층 FC (128→64→1)** 🆕
- 총 파라미터: 443,265개

**학습 설정**:
- 배치 크기: **64** ⬆️
- 학습률: 0.0005
- **L2 정규화 (weight_decay): 0.0001** 🆕
- **그래디언트 클리핑: 1.0** 🆕
- 최대 에포크: 150
- 조기 종료 patience: 20

**성능 결과**:
| 데이터셋 | Loss | RMSE | MAE | R² |
|---------|------|------|-----|-----|
| Train | 0.2077 | 39.01 | 23.28 | **0.7939** |
| Validation | 0.5450 | 63.41 | 36.90 | **0.3786** |
| Test | 0.5144 | 61.52 | 39.18 | **0.3597** |

**특징**:
- ✅ **가장 긴 학습 (90 에포크)** - 안정적인 학습
- ✅ **Validation-Test 성능 일관성** (R²: 0.3786 vs 0.3597)
- ✅ 과적합 완화 (Train-Test 격차 감소)
- ⚠️ 전반적인 성능은 실험 1보다 낮음
- ⚠️ 정규화가 너무 강해 underfitting 가능성

---

## 📈 실험 비교 분석

### 1. 성능 비교 (Test R²)

```
실험 1 (Baseline):     0.4032  ████████
실험 2 (Deep):         0.0450  █
실험 3 (Regularized):  0.3597  ███████
```

### 2. 과적합 정도 (Train R² - Test R²)

```
실험 1: 0.8154 - 0.4032 = 0.4122  ████████
실험 2: 0.8577 - 0.0450 = 0.8127  ████████████████
실험 3: 0.7939 - 0.3597 = 0.4342  █████████
```

### 3. 학습 안정성 (에포크 수)

```
실험 1: 18 에포크   (빠른 수렴, 조기 종료)
실험 2: 26 에포크   (빠른 수렴, 조기 종료)
실험 3: 90 에포크   (안정적 학습, 긴 학습)
```

---

## 🎯 핵심 발견사항

### 1. 모델 복잡도의 역설
- **더 깊은 모델 ≠ 더 좋은 성능**
- 실험 2에서 파라미터를 5.5배 증가시켰지만 Test R²는 오히려 급격히 하락
- 시계열 데이터에서는 적절한 복잡도가 중요

### 2. 과적합 문제
- 모든 실험에서 Train 성능 > Test 성능
- 실험 2에서 가장 심각 (Train R²: 0.8577, Test R²: 0.0450)
- 데이터의 시간적 특성으로 인한 분포 차이 가능성

### 3. 정규화 효과
- 배치 정규화, 높은 드롭아웃, L2 정규화, 그래디언트 클리핑 적용
- 과적합은 완화되었으나 전반적인 성능도 하락
- **정규화와 성능 사이의 트레이드오프 존재**

### 4. Validation-Test 불일치
- 실험 1, 2에서 Validation과 Test 성능 격차 큼
- 실험 3에서는 일관성 개선 (0.3786 vs 0.3597)
- 시계열 데이터의 시간적 분포 변화 영향

---

## 💡 문제점 분석

### 1. 데이터 관련 이슈

**시간적 분포 변화**:
- Train(60%) → Val(20%) → Test(20%) 순차 분할
- 시간이 지남에 따라 데이터 패턴 변화 가능성
- Test 세트가 가장 최근 데이터로 학습 데이터와 분포 차이

**특성 수 과다**:
- 203개의 특성 사용
- 샘플 수(26,193) 대비 특성 수가 많아 차원의 저주 가능성
- 불필요한 특성이 노이즈로 작용할 수 있음

### 2. 모델 관련 이슈

**시퀀스 길이**:
- 현재 24 시간 스텝 사용
- 유량 예측에 최적인지 검증 필요
- 더 긴 또는 짧은 시퀀스 실험 필요

**단일 타겟 예측**:
- y_Q_in만 예측 (y_flowA + y_flowB)
- 개별 예측 후 합산하는 방식도 고려 가능

---

## 🔧 개선 방안

### 1. 데이터 전처리 개선 (우선순위: 높음)

**특성 선택**:
```python
# 상관관계 분석을 통한 중요 특성 선택
# PCA 또는 특성 중요도 기반 차원 축소
# 목표: 203개 → 50-100개로 축소
```

**시계열 교차 검증**:
```python
# TimeSeriesSplit 사용
# 여러 시간 구간에서 검증하여 일반화 성능 평가
```

**데이터 증강**:
```python
# 시간 이동, 노이즈 추가 등
# 학습 데이터 다양성 증가
```

### 2. 모델 아키텍처 개선 (우선순위: 중간)

**Attention 메커니즘 추가**:
```python
# LSTM + Attention
# 중요한 시간 스텝에 집중
```

**앙상블 모델**:
```python
# 여러 LSTM 모델의 예측 평균
# 또는 LSTM + 다른 모델(GRU, Transformer) 앙상블
```

**멀티태스크 학습**:
```python
# y_flowA, y_flowB 개별 예측 + y_Q_in 예측
# 보조 태스크로 학습 개선
```

### 3. 하이퍼파라미터 튜닝 (우선순위: 중간)

**시퀀스 길이 실험**:
- 12, 24, 48, 72 시간 스텝 비교

**학습률 스케줄러**:
```python
# ReduceLROnPlateau 또는 CosineAnnealingLR
# 학습 중 학습률 동적 조정
```

**배치 크기 실험**:
- 32, 64, 128 비교

### 4. 평가 방법 개선 (우선순위: 높음)

**시계열 교차 검증**:
```python
from sklearn.model_selection import TimeSeriesSplit
# 5-fold 시계열 교차 검증
# 더 신뢰할 수 있는 성능 평가
```

**추가 평가 지표**:
- MAPE (Mean Absolute Percentage Error)
- 피크 유량 예측 정확도
- 시간대별 성능 분석

---

## 📋 권장 사항

### 즉시 실행 가능한 개선

1. **특성 선택 수행**
   - 상관관계 분석으로 중요 특성 50-100개 선택
   - 예상 효과: 과적합 감소, 학습 속도 향상

2. **시계열 교차 검증 적용**
   - TimeSeriesSplit으로 5-fold 검증
   - 예상 효과: 더 신뢰할 수 있는 성능 평가

3. **실험 1 모델 재검토**
   - 가장 균형잡힌 성능 (Test R²: 0.4032)
   - 약간의 정규화 추가로 개선 가능성

### 중장기 개선 방향

1. **Attention 기반 모델 시도**
   - LSTM + Attention 또는 Transformer
   - 시계열의 중요 패턴 학습 개선

2. **앙상블 접근**
   - 여러 모델의 예측 결합
   - 안정성과 성능 향상

3. **도메인 지식 활용**
   - 강우량, 시간대 등 도메인 특성 강화
   - 물리적 제약 조건 반영

---

## 📊 최종 결론

### 현재 최적 모델
**실험 1 (Baseline LSTM)** 이 가장 실용적:
- Test R²: 0.4032 (중간 수준)
- 적절한 복잡도 (102K 파라미터)
- 빠른 학습 (18 에포크)
- 과적합이 있지만 관리 가능한 수준

### 성능 한계 원인
1. **시간적 분포 변화**: Train-Test 간 데이터 패턴 차이
2. **특성 과다**: 203개 특성으로 인한 노이즈
3. **단순 분할**: 단일 시간 구간 분할의 한계

### 다음 단계
1. ✅ **특성 선택 및 차원 축소** (최우선)
2. ✅ **시계열 교차 검증** (최우선)
3. ⏭️ Attention 메커니즘 추가
4. ⏭️ 앙상블 모델 구축

---

**보고서 작성일**: 2026-01-29  
**실험 환경**: CUDA GPU, PyTorch  
**데이터셋**: modelFLOW_dataset.csv (26,193 samples, 203 features)
