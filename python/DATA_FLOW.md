# scripts/train.py ë°ì´í„° ì²˜ë¦¬ ë° ì˜ˆì¸¡ ì§„í–‰ ìˆœì„œ

## ğŸ“‹ ê°œìš”

`scripts/train.py`ëŠ” 3ê°€ì§€ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤:
1. **ê¸°ë³¸ íŒŒì´í”„ë¼ì¸** (`run_pipeline`)
2. **ê°œì„  íŒŒì´í”„ë¼ì¸** (`run_improved_pipeline`) - Optuna, í”¼ì²˜ ì„ íƒ, Scaling
3. **Sliding Window íŒŒì´í”„ë¼ì¸** (`run_sliding_window_pipeline`) - ì‹œê³„ì—´ ìœˆë„ìš° ê¸°ë°˜

---

## ğŸ”„ ê³µí†µ ì´ˆê¸° ë‹¨ê³„ (train.py)

### 1ë‹¨ê³„: CLI ì¸ì íŒŒì‹±
```bash
python scripts/train.py --mode flow --improved --n-features 50
```

**ì£¼ìš” ì¸ì:**
- `--mode`: ì˜ˆì¸¡ ëª¨ë“œ (flow/tms/modelA/modelB/modelC)
- `--improved`: ê°œì„  íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
- `--sliding-window`: Sliding Window íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
- `--resample`: ë¦¬ìƒ˜í”Œë§ ê·œì¹™ (ê¸°ë³¸: 1h)
- `--n-features`: ì„ íƒí•  í”¼ì²˜ ê°œìˆ˜ (ê¸°ë³¸: 50)
- `--cv-splits`: êµì°¨ ê²€ì¦ ë¶„í•  ìˆ˜ (ê¸°ë³¸: 3)
- `--n-trials`: Optuna ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸: 50)

### 2ë‹¨ê³„: ë°ì´í„° ë¡œë“œ (src/io.py)
```python
# CSV íŒŒì¼ ë¡œë“œ
df_flow, df_tms, df_aws_368, df_aws_541, df_aws_569 = load_csvs(data_root)

# ì „ì²˜ë¦¬
df_flow = prep_flow(df_flow)      # FLOW ë°ì´í„° ì •ë¦¬
df_aws = prep_aws(...)             # AWS ê¸°ìƒ ë°ì´í„° ë³‘í•©

dfs = {"flow": df_flow, "tms": df_tms, "aws": df_aws}
```

**ë¡œë“œë˜ëŠ” ë°ì´í„°:**
- `FLOW_Actual.csv`: ìœ ì… ìœ ëŸ‰ ë°ì´í„° (Q_in, flow_TankA/B, level_TankA/B)
- `TMS_Actual.csv`: ìˆ˜ì§ˆ ë°ì´í„° (TOC, PH, SS, FLUX, TN, TP)
- `AWS_368.csv`, `AWS_541.csv`, `AWS_569.csv`: ê¸°ìƒ ë°ì´í„° (ì˜¨ë„, ìŠµë„, ê°•ìˆ˜ëŸ‰ ë“±)

### 3ë‹¨ê³„: ë¶„í•  ì„¤ì •
```python
split_cfg = SplitConfig(
    train_ratio=0.6,   # 60% í•™ìŠµ
    valid_ratio=0.2,   # 20% ê²€ì¦
    test_ratio=0.2     # 20% í…ŒìŠ¤íŠ¸
)
```

---

## ğŸ“Š íŒŒì´í”„ë¼ì¸ë³„ ìƒì„¸ ì²˜ë¦¬ ìˆœì„œ

## 1ï¸âƒ£ ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ (run_pipeline)

### ì „ì²˜ë¦¬ ìˆœì„œ
```
ì›ë³¸ ë°ì´í„°
    â†“
[1ë‹¨ê³„] ì‹œê°„ì¶• ì •í•©
    â†“
[2ë‹¨ê³„] ê²°ì¸¡ì¹˜ ë³´ê°„ (1ì°¨)
    â†“
[3ë‹¨ê³„] ì´ìƒì¹˜ ì²˜ë¦¬
    â†“
[2ë‹¨ê³„] ê²°ì¸¡ì¹˜ ì¬ë³´ê°„ (2ì°¨)
    â†“
[4ë‹¨ê³„] ë¦¬ìƒ˜í”Œë§
    â†“
[5ë‹¨ê³„] íŒŒìƒ íŠ¹ì„± ìƒì„±
    â†“
[6ë‹¨ê³„] Train/Valid/Test ë¶„ë¦¬
    â†“
[7ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
```

### ìƒì„¸ ì„¤ëª…

#### [1ë‹¨ê³„] ì‹œê°„ì¶• ì •í•© (src/io.py)
```python
# ê° ë°ì´í„°í”„ë ˆì„ì„ DatetimeIndexë¡œ ë³€í™˜
dfs_indexed = {}
for name, df in dfs.items():
    if not isinstance(df.index, pd.DatetimeIndex):
        df = set_datetime_index(df, time_col=time_col_map[name])
    df = df.sort_index()  # ì‹œê°„ìˆœ ì •ë ¬
    df = df[~df.index.duplicated(keep='first')]  # ì¤‘ë³µ ì œê±°
    dfs_indexed[name] = df

# ë°ì´í„° ë³‘í•© (outer join)
df_all = merge_sources_on_time(dfs_indexed, how="outer")
```

**ê²°ê³¼:** ëª¨ë“  ë°ì´í„°ê°€ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë³‘í•©ë¨

#### [2ë‹¨ê³„] ê²°ì¸¡ì¹˜ ë³´ê°„ (src/preprocess.py)
```python
# ImputationConfig ì„¤ì •
config = ImputationConfig(
    method="ffill",           # Forward Fill
    ewma_span=12,            # EWMA ìœˆë„ìš° í¬ê¸°
    max_gap_hours=6,         # ìµœëŒ€ ë³´ê°„ ê°„ê²©
    use_ewma_for_long_gaps=True
)

df_all = impute_missing_with_strategy(df_all, freq="1h", config=config)
```

**ì „ëµ:**
- ì§§ì€ ê²°ì¸¡ (â‰¤6ì‹œê°„): Forward Fill
- ê¸´ ê²°ì¸¡ (>6ì‹œê°„): EWMA (Exponential Weighted Moving Average)
- ë³´ê°„ ë§ˆìŠ¤í¬ ì¶”ê°€ (`_imputed` ì»¬ëŸ¼)

#### [3ë‹¨ê³„] ì´ìƒì¹˜ ì²˜ë¦¬ (src/preprocess.py)
```python
# OutlierConfig ì„¤ì •
config = OutlierConfig(
    method="iqr",            # IQR ë°©ë²•
    iqr_multiplier=3.0,      # IQR * 3.0
    clip=False               # ì´ìƒì¹˜ë¥¼ NaNìœ¼ë¡œ ë³€í™˜
)

df_all = detect_and_handle_outliers(df_all, config=config)
```

**ë°©ë²•:**
- IQR (Interquartile Range) ê¸°ë°˜ íƒì§€
- ì´ìƒì¹˜ â†’ NaN ë³€í™˜ â†’ ì¬ë³´ê°„
- ì´ìƒì¹˜ ë§ˆìŠ¤í¬ ì¶”ê°€ (`_outlier` ì»¬ëŸ¼)

#### [4ë‹¨ê³„] ë¦¬ìƒ˜í”Œë§ (src/preprocess.py)
```python
df_hourly = resample_hourly(df_all, rule="1h", agg="mean")
```

**ëª©ì :** ì‹œê°„ ê°„ê²© í†µì¼ (ì˜ˆ: 10ë¶„ â†’ 1ì‹œê°„)

#### [5ë‹¨ê³„] íŒŒìƒ íŠ¹ì„± ìƒì„± (src/features.py)
```python
# ëª¨ë“œì— ë”°ë¼ ì œì™¸í•  ì»¬ëŸ¼ ê²°ì •
exclude_cols = get_exclude_features(mode, target_cols)

df_feat = build_features(
    df_hourly=df_hourly,
    target_cols=target_cols,
    exclude_cols=exclude_cols,
    cfg=feature_cfg
)
```

**ìƒì„±ë˜ëŠ” íŠ¹ì„±:**

1. **Rolling í†µê³„ (ì´ë™ í‰ê· /í‘œì¤€í¸ì°¨)**
   - ìœˆë„ìš°: 3, 6, 12, 24ì‹œê°„
   - ì˜ˆ: `TA_368_roll_mean_3h`, `RN_368_roll_std_6h`

2. **Lag íŠ¹ì„± (ê³¼ê±° ê°’)**
   - ì‹œì°¨: 1, 3, 6, 12, 24ì‹œê°„
   - ì˜ˆ: `TA_368_lag_1h`, `HM_368_lag_6h`

3. **ì‹œê°„ íŠ¹ì„±**
   - `hour`: ì‹œê°„ (0-23)
   - `day_of_week`: ìš”ì¼ (0-6)
   - `month`: ì›” (1-12)
   - `is_weekend`: ì£¼ë§ ì—¬ë¶€ (0/1)

4. **ì°¨ë¶„ íŠ¹ì„±**
   - 1ì‹œê°„ ì°¨ë¶„: `TA_368_diff_1h`

**ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€:**
- FLOW ëª¨ë“œ: TMS ë°ì´í„° ì „ì²´ ì œì™¸ (ë¯¸ë˜ ì •ë³´)
- TMS ëª¨ë“œ: FLOW ë°ì´í„° ì œì™¸ (ë¯¸ë˜ ì •ë³´)
- ModelA/B/C: ì˜ˆì¸¡ ëŒ€ìƒë§Œ ì œì™¸, ë‚˜ë¨¸ì§€ TMSëŠ” ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©

#### [6ë‹¨ê³„] ì§€ë„í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± (src/features.py)
```python
X, y = make_supervised_dataset(
    df_feat, 
    target_cols=target_cols,
    exclude_cols=exclude_cols,
    dropna=True  # NaN í–‰ ì œê±°
)
```

**ê²°ê³¼:**
- `X`: ì…ë ¥ íŠ¹ì„± (ì˜ˆ: 200ê°œ í”¼ì²˜)
- `y`: íƒ€ê²Ÿ ë³€ìˆ˜ (ì˜ˆ: Q_in)

#### [7ë‹¨ê³„] ë°ì´í„° ë¶„í•  (src/split.py)
```python
splits = time_split(X, y, cfg=split_cfg)

X_train, y_train = splits["train"]  # 60%
X_valid, y_valid = splits["valid"]  # 20%
X_test, y_test = splits["test"]     # 20%
```

**ì‹œê³„ì—´ ë¶„í• :** ì‹œê°„ ìˆœì„œ ìœ ì§€ (ê³¼ê±° â†’ ë¯¸ë˜)

#### [8ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ (src/models.py)
```python
# ëª¨ë¸ Zoo ìƒì„±
zoo = build_model_zoo(random_state=42)
# í¬í•¨ ëª¨ë¸: RandomForest, XGBoost, HistGradientBoosting

# ê° ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
for model_name, model in zoo.items():
    model = wrap_multioutput_if_needed(model, y)  # ë‹¤ì¤‘ íƒ€ê²Ÿ ì²˜ë¦¬
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    y_pred_train = model.predict(X_train)
    y_pred_valid = model.predict(X_valid)
    y_pred_test = model.predict(X_test)
    
    # í‰ê°€
    metrics = compute_metrics(y_test, y_pred_test)
    # RÂ², RMSE, MAE, MAPE
```

---

## 2ï¸âƒ£ ê°œì„  íŒŒì´í”„ë¼ì¸ (run_improved_pipeline)

### ì „ì²˜ë¦¬ ìˆœì„œ
```
ì›ë³¸ ë°ì´í„°
    â†“
[1-5ë‹¨ê³„] ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼
    â†“
[6ë‹¨ê³„] Train/Valid/Test ë¶„ë¦¬
    â†“
[7ë‹¨ê³„] ìŠ¤ì¼€ì¼ë§ (Train ê¸°ì¤€) â­ ì¶”ê°€
    â†“
[8ë‹¨ê³„] í”¼ì²˜ ì„ íƒ (Train ê¸°ì¤€) â­ ì¶”ê°€
    â†“
[9ë‹¨ê³„] Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” â­ ì¶”ê°€
    â†“
ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
```

### ì¶”ê°€ ë‹¨ê³„ ìƒì„¸ ì„¤ëª…

#### [7ë‹¨ê³„] ìŠ¤ì¼€ì¼ë§ (src/scaling.py)
```python
from sklearn.preprocessing import StandardScaler

X_train_scaled, X_valid_scaled, X_test_scaled, scaler = scale_data(
    X_train, X_valid, X_test
)

# StandardScaler: (X - mean) / std
# Train ë°ì´í„°ë¡œ fit, Valid/TestëŠ” transformë§Œ
```

**ëª©ì :** íŠ¹ì„± ìŠ¤ì¼€ì¼ í†µì¼ (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)

**ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€:**
- Train ë°ì´í„°ë¡œë§Œ scaler fit
- Valid/TestëŠ” Trainì˜ í†µê³„ëŸ‰ ì‚¬ìš©

#### [8ë‹¨ê³„] í”¼ì²˜ ì„ íƒ (src/feature_selection.py)
```python
top_features = select_top_features(
    X_train_scaled, 
    y_train,
    n_features=50,  # ìƒìœ„ 50ê°œ ì„ íƒ
    random_state=42
)

X_train_selected = X_train_scaled[top_features]
X_valid_selected = X_valid_scaled[top_features]
X_test_selected = X_test_scaled[top_features]
```

**ë°©ë²•:**
1. RandomForestë¡œ feature importance ê³„ì‚°
2. ì¤‘ìš”ë„ ìƒìœ„ Nê°œ ì„ íƒ

**ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€:**
- Train ë°ì´í„°ë¡œë§Œ ì¤‘ìš”ë„ ê³„ì‚°
- Valid/TestëŠ” ì„ íƒëœ í”¼ì²˜ë§Œ ì‚¬ìš©

#### [9ë‹¨ê³„] Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (src/models.py)
```python
# Optuna ë˜í¼ ëª¨ë¸ ìƒì„±
zoo = build_model_zoo_with_optuna(
    cv_splits=3,      # TimeSeriesSplit
    n_trials=50,      # 50ë²ˆ ì‹œë„
    random_state=42
)

# ê° ëª¨ë¸ë³„ ìµœì í™”
for model_name, optuna_model in zoo.items():
    # Optunaê°€ ìë™ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
    optuna_model.fit(X_train_selected, y_train)
    
    # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì¬í•™ìŠµ
    best_params = optuna_model.best_params_
    
    # XGBoostëŠ” Early Stopping ì¶”ê°€
    if model_name == "XGBoost":
        final_model = xgb.XGBRegressor(**best_params)
        final_model.fit(
            X_train_selected, y_train,
            eval_set=[(X_valid_selected, y_valid)],
            early_stopping_rounds=20,
            verbose=False
        )
```

**ìµœì í™” ëŒ€ìƒ íŒŒë¼ë¯¸í„°:**

**XGBoost:**
- `max_depth`: 3-10
- `learning_rate`: 0.01-0.3
- `n_estimators`: 100-1000
- `subsample`: 0.6-1.0
- `colsample_bytree`: 0.6-1.0

**HistGradientBoosting:**
- `max_depth`: 3-15
- `learning_rate`: 0.01-0.3
- `max_iter`: 100-500

**RandomForest:**
- `n_estimators`: 100-500
- `max_depth`: 10-50
- `min_samples_split`: 2-20

---

## 3ï¸âƒ£ Sliding Window íŒŒì´í”„ë¼ì¸ (run_sliding_window_pipeline)

### ì „ì²˜ë¦¬ ìˆœì„œ
```
ì›ë³¸ ë°ì´í„°
    â†“
[1-5ë‹¨ê³„] ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼
    â†“
[6ë‹¨ê³„] Sliding Window ìƒì„± â­ ì¶”ê°€
    â†“
[7ë‹¨ê³„] ìœˆë„ìš° ë‹¨ìœ„ ë°ì´í„° ë¶„í• 
    â†“
[8ë‹¨ê³„] ìœˆë„ìš° í‰íƒ„í™” (2D ë³€í™˜)
    â†“
[9ë‹¨ê³„] ìŠ¤ì¼€ì¼ë§
    â†“
[10ë‹¨ê³„] í”¼ì²˜ ì„ íƒ
    â†“
ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
```

### ì¶”ê°€ ë‹¨ê³„ ìƒì„¸ ì„¤ëª…

#### [6ë‹¨ê³„] Sliding Window ìƒì„± (src/sliding_window.py)
```python
X_seq, y_seq = create_sliding_windows(
    X, y,
    window_size=24,  # ê³¼ê±° 24ì‹œê°„
    horizon=1,       # 1ì‹œê°„ í›„ ì˜ˆì¸¡
    stride=1         # 1ì‹œê°„ì”© ì´ë™
)
```

**ë³€í™˜:**
```
ì›ë³¸ ë°ì´í„° (2D):
X: (10000 ìƒ˜í”Œ, 200 í”¼ì²˜)
y: (10000 ìƒ˜í”Œ, 1 íƒ€ê²Ÿ)

â†“ Sliding Window

3D ì‹œí€€ìŠ¤ ë°ì´í„°:
X_seq: (9975 ìœˆë„ìš°, 24 ì‹œê°„, 200 í”¼ì²˜)
y_seq: (9975 ìœˆë„ìš°, 1 íƒ€ê²Ÿ)
```

**ì˜ˆì‹œ:**
```
ìœˆë„ìš° 1: [ì‹œê°„ 0-23] â†’ ì‹œê°„ 24 ì˜ˆì¸¡
ìœˆë„ìš° 2: [ì‹œê°„ 1-24] â†’ ì‹œê°„ 25 ì˜ˆì¸¡
ìœˆë„ìš° 3: [ì‹œê°„ 2-25] â†’ ì‹œê°„ 26 ì˜ˆì¸¡
...
```

#### [8ë‹¨ê³„] ìœˆë„ìš° í‰íƒ„í™” (src/sliding_window.py)
```python
# 3D â†’ 2D ë³€í™˜ (ML ëª¨ë¸ìš©)
X_train_flat = flatten_windows_for_ml(X_train_seq)

# (9975, 24, 200) â†’ (9975, 4800)
# 24ì‹œê°„ * 200í”¼ì²˜ = 4800 í”¼ì²˜
```

**íŠ¹ì„± ì´ë¦„ ìƒì„±:**
```python
feature_names = create_feature_names_for_flattened_windows(
    original_features, 
    window_size=24
)

# ì˜ˆ: ['TA_368_t-23', 'TA_368_t-22', ..., 'TA_368_t-0']
```

---

## ğŸ“ˆ ëª¨ë¸ í‰ê°€ ì§€í‘œ

### ê³„ì‚°ë˜ëŠ” ì§€í‘œ (src/metrics.py)
```python
metrics = compute_metrics(y_true, y_pred)

# ë°˜í™˜ê°’:
{
    "R2_mean": 0.85,           # ê²°ì •ê³„ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
    "RMSE_mean": 12.5,         # í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    "MAE_mean": 8.3,           # í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    "MAPE_mean(%)": 5.2        # í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
}
```

### ì§€í‘œ ì˜ë¯¸
- **RÂ² (ê²°ì •ê³„ìˆ˜)**: ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ê°€ (0-1, 1ì´ ì™„ë²½)
- **RMSE**: ì˜ˆì¸¡ ì˜¤ì°¨ì˜ í¬ê¸° (íƒ€ê²Ÿê³¼ ê°™ì€ ë‹¨ìœ„)
- **MAE**: ì ˆëŒ€ ì˜¤ì°¨ì˜ í‰ê·  (ì´ìƒì¹˜ì— ëœ ë¯¼ê°)
- **MAPE**: ë°±ë¶„ìœ¨ ì˜¤ì°¨ (ìƒëŒ€ì  ì„±ëŠ¥ í‰ê°€)

---

## ğŸ’¾ ê²°ê³¼ ì €ì¥

### ì €ì¥ë˜ëŠ” íŒŒì¼ (src/save_results.py)
```
results/ML/
â”œâ”€â”€ predictions/
â”‚   â”œâ”€â”€ {mode}_train_predictions.csv      # í•™ìŠµ ë°ì´í„° ì˜ˆì¸¡ê°’
â”‚   â”œâ”€â”€ {mode}_valid_predictions.csv      # ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ê°’
â”‚   â””â”€â”€ {mode}_test_predictions.csv       # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’
â”œâ”€â”€ sequences/                             # Sliding Windowë§Œ
â”‚   â”œâ”€â”€ {mode}_X_seq.npz                  # 3D ì‹œí€€ìŠ¤ ë°ì´í„°
â”‚   â””â”€â”€ {mode}_y_seq.npz
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ {model_name}_{mode}.pkl           # í•™ìŠµëœ ëª¨ë¸
â”‚   â””â”€â”€ scaler_{mode}.pkl                 # ìŠ¤ì¼€ì¼ëŸ¬
â”œâ”€â”€ {mode}_r2_comparison.png              # RÂ² ë¹„êµ ê·¸ë˜í”„
â”œâ”€â”€ {mode}_{model}_learning_curve.png     # í•™ìŠµ ê³¡ì„ 
â””â”€â”€ analysis_report.md                     # ë¶„ì„ ë³´ê³ ì„œ
```

---

## ğŸ¯ ëª¨ë“œë³„ ì˜ˆì¸¡ ëŒ€ìƒ ë° ì…ë ¥ ë°ì´í„°

### Flow ëª¨ë“œ (ìœ ëŸ‰ ì˜ˆì¸¡)
```python
mode = "flow"
target = ["Q_in"]  # ìœ ì… ìœ ëŸ‰

# ì…ë ¥ ë°ì´í„°
inputs = [
    "AWS ê¸°ìƒ ë°ì´í„°",      # TA, HM, RN, WS, WD ë“±
    "level_TankA",          # íƒ±í¬ A ìˆ˜ìœ„
    "level_TankB"           # íƒ±í¬ B ìˆ˜ìœ„
]

# ì œì™¸ ë°ì´í„° (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
excluded = [
    "ëª¨ë“  TMS ì§€í‘œ",        # ë¯¸ë˜ ì •ë³´
    "flow_TankA",           # Q_inì˜ êµ¬ì„± ìš”ì†Œ
    "flow_TankB"            # Q_inì˜ êµ¬ì„± ìš”ì†Œ
]
```

### TMS ëª¨ë“œ (ì „ì²´ ìˆ˜ì§ˆ ì˜ˆì¸¡)
```python
mode = "tms"
targets = ["TOC_VU", "PH_VU", "SS_VU", "FLUX_VU", "TN_VU", "TP_VU"]

# ì…ë ¥ ë°ì´í„°
inputs = ["AWS ê¸°ìƒ ë°ì´í„°"]

# ì œì™¸ ë°ì´í„°
excluded = ["ëª¨ë“  FLOW ë°ì´í„°"]  # ë¯¸ë˜ ì •ë³´
```

### ModelA (ìœ ê¸°ë¬¼/ì…ì ì˜ˆì¸¡)
```python
mode = "modelA"
targets = ["TOC_VU", "SS_VU"]  # ìœ ê¸°ë¬¼, ë¶€ìœ ë¬¼ì§ˆ

# ì…ë ¥ ë°ì´í„°
inputs = [
    "AWS ê¸°ìƒ ë°ì´í„°",
    "PH_VU", "FLUX_VU", "TN_VU", "TP_VU"  # ë‚˜ë¨¸ì§€ TMS ì§€í‘œ
]

# ì œì™¸ ë°ì´í„°
excluded = [
    "TOC_VU", "SS_VU",      # ì˜ˆì¸¡ ëŒ€ìƒ
    "ëª¨ë“  FLOW ë°ì´í„°"       # ë¯¸ë˜ ì •ë³´
]
```

### ModelB (ì˜ì–‘ì—¼ ì˜ˆì¸¡)
```python
mode = "modelB"
targets = ["TN_VU", "TP_VU"]  # ì´ì§ˆì†Œ, ì´ì¸

# ì…ë ¥ ë°ì´í„°
inputs = [
    "AWS ê¸°ìƒ ë°ì´í„°",
    "TOC_VU", "PH_VU", "SS_VU", "FLUX_VU"  # ë‚˜ë¨¸ì§€ TMS ì§€í‘œ
]

# ì œì™¸ ë°ì´í„°
excluded = [
    "TN_VU", "TP_VU",       # ì˜ˆì¸¡ ëŒ€ìƒ
    "ëª¨ë“  FLOW ë°ì´í„°"       # ë¯¸ë˜ ì •ë³´
]
```

### ModelC (ê³µì • ìƒíƒœ ì˜ˆì¸¡)
```python
mode = "modelC"
targets = ["FLUX_VU", "PH_VU"]  # ìœ ëŸ‰ê³„, pH

# ì…ë ¥ ë°ì´í„°
inputs = [
    "AWS ê¸°ìƒ ë°ì´í„°",
    "TOC_VU", "SS_VU", "TN_VU", "TP_VU"  # ë‚˜ë¨¸ì§€ TMS ì§€í‘œ
]

# ì œì™¸ ë°ì´í„°
excluded = [
    "FLUX_VU", "PH_VU",     # ì˜ˆì¸¡ ëŒ€ìƒ
    "ëª¨ë“  FLOW ë°ì´í„°"       # ë¯¸ë˜ ì •ë³´
]
```

---

## ğŸš€ ì‹¤í–‰ ì˜ˆì‹œ

### 1. ê¸°ë³¸ íŒŒì´í”„ë¼ì¸
```bash
python scripts/train.py --mode flow --resample 1h
```

### 2. ê°œì„  íŒŒì´í”„ë¼ì¸ (ê¶Œì¥)
```bash
python scripts/train.py \
    --mode flow \
    --improved \
    --n-features 50 \
    --cv-splits 3 \
    --n-trials 50 \
    --resample 1h
```

### 3. Sliding Window íŒŒì´í”„ë¼ì¸
```bash
python scripts/train.py \
    --mode flow \
    --sliding-window \
    --improved \
    --window-size 24 \
    --horizon 1 \
    --n-features 50
```

---

## ğŸ“Š ì¶œë ¥ ì˜ˆì‹œ

```
============================================================
WWTP ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ (ê°œì„  íŒŒì´í”„ë¼ì¸)
============================================================
ëª¨ë“œ: FLOW
ë°ì´í„° ê²½ë¡œ: data/actual
ë¦¬ìƒ˜í”Œë§: 1h
í”¼ì²˜ ì„ íƒ: ìƒìœ„ 50ê°œ
êµì°¨ ê²€ì¦: 3 splits
Optuna ì‹œë„: 50 trials
============================================================

[1/8] ë°ì´í„° ë¡œë“œ ì¤‘...

[3/8] ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...

============================================================
ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ (Optuna) - ëª¨ë“œ: FLOW
============================================================

[1/9] ì‹œê°„ì¶• ì •í•© ì¤‘...
[2/9] ê²°ì¸¡ì¹˜ ë³´ê°„ ì¤‘ (1ì°¨)...
[3/9] ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬ ì¤‘...
[4/9] ë¦¬ìƒ˜í”Œë§ ì¤‘ (1h)...
[5/9] íŒŒìƒ íŠ¹ì„± ìƒì„± ì¤‘...
ë°ì´í„°ì…‹ í¬ê¸°: 26193 ìƒ˜í”Œ, 203 í”¼ì²˜
[6/9] ë°ì´í„° ë¶„í•  ì¤‘...
  Train: 15715 ìƒ˜í”Œ
  Valid: 5238 ìƒ˜í”Œ
  Test:  5240 ìƒ˜í”Œ
[7/9] ìŠ¤ì¼€ì¼ë§ ì¤‘ (Train ê¸°ì¤€)...
[8/9] íŠ¹ì„± ì„ íƒ ì¤‘ (ìƒìœ„ 50ê°œ, Train ê¸°ì¤€)...
  ì„ íƒëœ í”¼ì²˜: 50ê°œ
[9/9] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì¤‘...

============================================================
ëª¨ë¸ í•™ìŠµ: XGBoost
Optuna ìµœì í™” ì¤‘ (50 trials)...
============================================================
  ìµœì  íŒŒë¼ë¯¸í„°: {'max_depth': 7, 'learning_rate': 0.05, ...}
  ìµœì  MSE: 125.34
  Early stopping: 234ë²ˆì§¸ ë°˜ë³µ

  Train - RÂ²: 0.9234, RMSE: 8.52
  Valid - RÂ²: 0.8567, RMSE: 11.23
  Test  - RÂ²: 0.8432, RMSE: 12.15

============================================================
ìµœì¢… ê²°ê³¼ (Test Set)
============================================================
     model  R2_mean  RMSE_mean  MAPE_mean(%)
   XGBoost   0.8432      12.15          4.23
   HistGBR   0.8201      13.45          4.87
RandomForest 0.7856      15.23          5.45

============================================================
ìµœê³  ì„±ëŠ¥ ëª¨ë¸
============================================================
ëª¨ë¸: XGBoost
Test RÂ²: 0.8432
Test RMSE: 12.15

ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: results/ML

ì €ì¥ëœ íŒŒì¼:
  ğŸ“Š ì˜ˆì¸¡ê°’: 3ê°œ íŒŒì¼
  ğŸ¤– ëª¨ë¸: 3ê°œ íŒŒì¼

============================================================
í•™ìŠµ ì™„ë£Œ!
============================================================
```

---

## ğŸ” í•µì‹¬ í¬ì¸íŠ¸

### ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€
1. **ì‹œê°„ ìˆœì„œ ìœ ì§€**: Train â†’ Valid â†’ Test (ê³¼ê±° â†’ ë¯¸ë˜)
2. **ìŠ¤ì¼€ì¼ë§**: Trainìœ¼ë¡œë§Œ fit, Valid/TestëŠ” transform
3. **í”¼ì²˜ ì„ íƒ**: Trainìœ¼ë¡œë§Œ ì¤‘ìš”ë„ ê³„ì‚°
4. **ë¯¸ë˜ ì •ë³´ ì œì™¸**: 
   - FLOW ì˜ˆì¸¡ ì‹œ TMS ë°ì´í„° ì œì™¸
   - TMS ì˜ˆì¸¡ ì‹œ FLOW ë°ì´í„° ì œì™¸

### ì„±ëŠ¥ í–¥ìƒ ê¸°ë²•
1. **ê²°ì¸¡ì¹˜ ì²˜ë¦¬**: Forward Fill + EWMA
2. **ì´ìƒì¹˜ ì²˜ë¦¬**: IQR ê¸°ë°˜ íƒì§€ ë° ì œê±°
3. **íŒŒìƒ íŠ¹ì„±**: Rolling, Lag, ì‹œê°„ íŠ¹ì„±
4. **í”¼ì²˜ ì„ íƒ**: ì¤‘ìš”ë„ ê¸°ë°˜ ìƒìœ„ Nê°œ ì„ íƒ
5. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**: Optuna + TimeSeriesSplit
6. **Early Stopping**: XGBoost ê³¼ì í•© ë°©ì§€

### ì‹œê³„ì—´ íŠ¹í™”
1. **ì‹œê°„ ìˆœì„œ ë¶„í• **: ê³¼ê±° ë°ì´í„°ë¡œ ë¯¸ë˜ ì˜ˆì¸¡
2. **Sliding Window**: ê³¼ê±° Nì‹œê°„ â†’ ë¯¸ë˜ ì˜ˆì¸¡
3. **Lag íŠ¹ì„±**: ê³¼ê±° ê°’ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
4. **Rolling í†µê³„**: ì´ë™ í‰ê· /í‘œì¤€í¸ì°¨
