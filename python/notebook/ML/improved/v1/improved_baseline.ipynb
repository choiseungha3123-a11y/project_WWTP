{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55560bb",
   "metadata": {},
   "source": [
    "Improved ML Baseline for WWTP Prediction (V1)\n",
    "\n",
    "ê°œì„ ì‚¬í•­:\n",
    "\n",
    "1. ê²°ì¸¡ì¹˜ ì œê±° (dropna)\n",
    "2. StandardScaler ì ìš©\n",
    "3. GridSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "4. í”¼ì²˜ ì„ íƒ (ì¤‘ìš”ë„ ê¸°ë°˜)\n",
    "5. TimeSeriesSplit êµì°¨ ê²€ì¦\n",
    "6. XGBoost ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d877db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "RESULTS_DIR = \"../../../../results/ML/v1\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Target config\n",
    "# =========================\n",
    "TARGETS_FLOW = [\"Q_in\"]\n",
    "TARGETS_TMS = [\"TOC_VU\", \"PH_VU\", \"SS_VU\", \"FLUX_VU\", \"TN_VU\", \"TP_VU\"]\n",
    "TARGETS_ALL = TARGETS_FLOW + TARGETS_TMS\n",
    "\n",
    "def get_target_cols(mode):\n",
    "    mode = mode.lower().strip()\n",
    "    if mode == \"flow\":\n",
    "        return TARGETS_FLOW\n",
    "    if mode == \"tms\":\n",
    "        return TARGETS_TMS\n",
    "    if mode == \"all\":\n",
    "        return TARGETS_ALL\n",
    "    raise ValueError(\"mode must be one of: 'flow', 'tms', 'all'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97483761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Time index & merge\n",
    "# =========================\n",
    "def set_datetime_index(df, time_col, tz=None):\n",
    "    out = df.copy()\n",
    "    out[time_col] = pd.to_datetime(out[time_col], errors=\"coerce\")\n",
    "    out = out.dropna(subset=[time_col]).set_index(time_col).sort_index()\n",
    "    return out\n",
    "\n",
    "def merge_sources_on_time(dfs, how=\"outer\"):\n",
    "    items = [df.copy() for df in dfs.values() if df is not None and len(df) > 0]\n",
    "    if not items:\n",
    "        raise ValueError(\"No non-empty dataframes to merge.\")\n",
    "    out = items[0]\n",
    "    for nxt in items[1:]:\n",
    "        out = out.join(nxt, how=how)\n",
    "    out = out.sort_index()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Cleaning & resample (V1: ê²°ì¸¡ì¹˜ëŠ” dropnaë¡œ ì œê±°)\n",
    "# =========================\n",
    "def resample_hourly(df, rule=\"1h\", agg=\"mean\"):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"df must have a DatetimeIndex for resampling.\")\n",
    "    out = df.copy()\n",
    "    if isinstance(agg, str):\n",
    "        return out.resample(rule).agg(agg)\n",
    "    return out.resample(rule).agg(agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fa815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Feature Engineering (V1: ì‹œê°„ íŠ¹ì„± + lag + rollingë§Œ)\n",
    "# =========================\n",
    "def add_time_features(df, add_sin_cos=True):\n",
    "    out = df.copy()\n",
    "    idx = out.index\n",
    "    if not isinstance(idx, pd.DatetimeIndex):\n",
    "        raise ValueError(\"df must have a DatetimeIndex for time features.\")\n",
    "    \n",
    "    out[\"hour\"] = idx.hour\n",
    "    out[\"dayofweek\"] = idx.dayofweek\n",
    "    out[\"month\"] = idx.month\n",
    "    out[\"is_weekend\"] = (idx.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # Season\n",
    "    m = out[\"month\"]\n",
    "    season_map = {12:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3}\n",
    "    out[\"season\"] = m.map(season_map).astype(int)\n",
    "    \n",
    "    # Season\n",
    "    m = out[\"month\"]\n",
    "    season_map = {12:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3}\n",
    "    out[\"season\"] = m.map(season_map).astype(int)\n",
    "    \n",
    "    if add_sin_cos:\n",
    "        out[\"sin_hour\"] = np.sin(2 * np.pi * out[\"hour\"] / 24.0)\n",
    "        out[\"cos_hour\"] = np.cos(2 * np.pi * out[\"hour\"] / 24.0)\n",
    "        out[\"sin_dow\"] = np.sin(2 * np.pi * out[\"dayofweek\"] / 7.0)\n",
    "        out[\"cos_dow\"] = np.cos(2 * np.pi * out[\"dayofweek\"] / 7.0)\n",
    "        out[\"sin_month\"] = np.sin(2 * np.pi * out[\"month\"] / 12.0)\n",
    "        out[\"cos_month\"] = np.cos(2 * np.pi * out[\"month\"] / 12.0)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbe5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(df, base_cols, lags):\n",
    "    out = df.copy()\n",
    "    for c in base_cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        for k in lags:\n",
    "            out[f\"{c}_lag{k}\"] = out[c].shift(k)\n",
    "    return out\n",
    "\n",
    "def add_rolling_features(df, base_cols, windows, stats=[\"mean\"]):\n",
    "    out = df.copy()\n",
    "    for c in base_cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        for w in windows:\n",
    "            r = out[c].rolling(window=w, min_periods=w)\n",
    "            if \"mean\" in stats:\n",
    "                out[f\"{c}_r{w}_mean\"] = r.mean()\n",
    "            if \"std\" in stats:\n",
    "                out[f\"{c}_r{w}_std\"] = r.std()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    add_time = True\n",
    "    add_sin_cos = True\n",
    "    lag_hours = None\n",
    "    roll_hours = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.lag_hours is None:\n",
    "            self.lag_hours = [1, 3, 6, 12, 24]\n",
    "        if self.roll_hours is None:\n",
    "            self.roll_hours = [3, 12, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc6e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(df_hourly, target_cols, feature_base_cols=None, cfg=FeatureConfig()):\n",
    "    out = df_hourly.copy()\n",
    "    \n",
    "    if cfg.add_time:\n",
    "        out = add_time_features(out, add_sin_cos=cfg.add_sin_cos)\n",
    "    \n",
    "    if feature_base_cols is None:\n",
    "        numeric_cols = out.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        feature_base_cols = [c for c in numeric_cols if c not in target_cols]\n",
    "    \n",
    "    out = add_lag_features(out, base_cols=feature_base_cols, lags=cfg.lag_hours)\n",
    "    out = add_rolling_features(out, base_cols=feature_base_cols, windows=cfg.roll_hours)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised_dataset(df, target_cols):\n",
    "    \"\"\"ê²°ì¸¡ì¹˜ ì œê±° í¬í•¨\"\"\"\n",
    "    missing = [c for c in target_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"target_cols not found in df: {missing}\")\n",
    "    \n",
    "    y = df[target_cols].copy()\n",
    "    X = df.drop(columns=target_cols).copy()\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ì œê±°\n",
    "    keep = X.notna().all(axis=1) & y.notna().all(axis=1)\n",
    "    X_clean = X.loc[keep]\n",
    "    y_clean = y.loc[keep]\n",
    "    \n",
    "    print(f\"Original samples: {len(X)}, After dropna: {len(X_clean)} ({len(X_clean)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    return X_clean, y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd87978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Split (time-based)\n",
    "# =========================\n",
    "@dataclass\n",
    "class SplitConfig:\n",
    "    train_ratio = 0.6\n",
    "    valid_ratio = 0.2\n",
    "    test_ratio = 0.2\n",
    "\n",
    "def time_split(X, y, cfg=SplitConfig()):\n",
    "    n = len(X)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Empty dataset after preprocessing/feature generation.\")\n",
    "    \n",
    "    n_train = int(n * cfg.train_ratio)\n",
    "    n_valid = int(n * cfg.valid_ratio)\n",
    "    \n",
    "    X_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\n",
    "    X_valid, y_valid = X.iloc[n_train:n_train+n_valid], y.iloc[n_train:n_train+n_valid]\n",
    "    X_test, y_test = X.iloc[n_train+n_valid:], y.iloc[n_train+n_valid:]\n",
    "    \n",
    "    return {\n",
    "        \"train\": (X_train, y_train),\n",
    "        \"valid\": (X_valid, y_valid),\n",
    "        \"test\": (X_test, y_test),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78575b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5) Feature Selection\n",
    "# =========================\n",
    "def select_top_features(X_train, y_train, n_features=50):\n",
    "    \"\"\"RandomForestë¡œ í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚° í›„ ìƒìœ„ nê°œ ì„ íƒ\"\"\"\n",
    "    print(f\"\\ní”¼ì²˜ ì„ íƒ ì¤‘... (ì´ {X_train.shape[1]}ê°œ â†’ ìƒìœ„ {n_features}ê°œ)\")\n",
    "    \n",
    "    # ë‹¨ì¼ íƒ€ê²Ÿì¸ ê²½ìš°\n",
    "    if len(y_train.shape) == 1 or y_train.shape[1] == 1:\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train.values.ravel() if hasattr(y_train, 'values') else y_train)\n",
    "        importances = rf.feature_importances_\n",
    "    else:\n",
    "        # ë‹¤ì¤‘ íƒ€ê²Ÿì¸ ê²½ìš° í‰ê·  ì¤‘ìš”ë„ ì‚¬ìš©\n",
    "        rf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "        rf.fit(X_train, y_train)\n",
    "        importances = np.mean([est.feature_importances_ for est in rf.estimators_], axis=0)\n",
    "    \n",
    "    # ìƒìœ„ nê°œ í”¼ì²˜ ì„ íƒ\n",
    "    top_indices = np.argsort(importances)[-n_features:]\n",
    "    top_features = X_train.columns[top_indices].tolist()\n",
    "    \n",
    "    print(f\"ì„ íƒëœ ìƒìœ„ 10ê°œ í”¼ì²˜: {top_features[-10:]}\")\n",
    "    \n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0321fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6) Models with GridSearch\n",
    "# =========================\n",
    "def build_model_zoo_with_gridsearch(n_targets=1, cv=3):\n",
    "    \"\"\"GridSearchCVë¥¼ í¬í•¨í•œ ëª¨ë¸ ì •ì˜ (Early Stopping ì§€ì›)\"\"\"\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=cv)\n",
    "    \n",
    "    # Ridge\n",
    "    ridge_params = {\n",
    "        'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "    ridge = GridSearchCV(\n",
    "        Ridge(random_state=42),\n",
    "        ridge_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Lasso\n",
    "    lasso_params = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "    }\n",
    "    lasso = GridSearchCV(\n",
    "        Lasso(random_state=42, max_iter=5000),\n",
    "        lasso_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # RandomForest (Early Stopping ì—†ìŒ, n_estimatorsë¡œ ì œì–´)\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "    rf = GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        rf_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # HistGradientBoosting (Early Stopping ì§€ì›)\n",
    "    hgb_params = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_iter': [500],  # ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •\n",
    "        'max_depth': [5, 10, 20],\n",
    "        'early_stopping': [True],\n",
    "        'n_iter_no_change': [20],  # 20ë²ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨\n",
    "        'validation_fraction': [0.2]\n",
    "    }\n",
    "    hgb = GridSearchCV(\n",
    "        HistGradientBoostingRegressor(random_state=42),\n",
    "        hgb_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # XGBoost (Early Stoppingì€ GridSearch ì™¸ë¶€ì—ì„œ ì²˜ë¦¬)\n",
    "    xgb_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    xgb_model = GridSearchCV(\n",
    "        xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "        xgb_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    zoo = {\n",
    "        \"Ridge\": ridge,\n",
    "        \"Lasso\": lasso,\n",
    "        \"RandomForest\": rf,\n",
    "        \"HistGBR\": hgb,\n",
    "        \"XGBoost\": xgb_model,\n",
    "    }\n",
    "    \n",
    "    return zoo\n",
    "\n",
    "def wrap_multioutput_if_needed(model, y):\n",
    "    \"\"\"ë‹¤ì¤‘ íƒ€ê²Ÿì¸ ê²½ìš° ê°œë³„ ëª¨ë¸ë¡œ ì²˜ë¦¬ (MultiOutputRegressor ì‚¬ìš© ì•ˆ í•¨)\"\"\"\n",
    "    # ê°œë³„ íƒ€ê²Ÿë³„ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ë¯€ë¡œ ë˜í•‘ ë¶ˆí•„ìš”\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7) Metrics & Evaluation\n",
    "# =========================\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    yt = np.asarray(y_true)\n",
    "    yp = np.asarray(y_pred)\n",
    "    if yt.ndim == 1:\n",
    "        yt = yt.reshape(-1, 1)\n",
    "    if yp.ndim == 1:\n",
    "        yp = yp.reshape(-1, 1)\n",
    "    \n",
    "    r2s, rmses, mapes = [], [], []\n",
    "    for j in range(yt.shape[1]):\n",
    "        r2s.append(r2_score(yt[:, j], yp[:, j]))\n",
    "        rmses.append(math.sqrt(mean_squared_error(yt[:, j], yp[:, j])))\n",
    "        mapes.append(mean_absolute_percentage_error(yt[:, j], yp[:, j]) * 100.0)\n",
    "    \n",
    "    return {\n",
    "        \"R2_mean\": float(np.mean(r2s)),\n",
    "        \"RMSE_mean\": float(np.mean(rmses)),\n",
    "        \"MAPE_mean(%)\": float(np.mean(mapes)),\n",
    "        \"R2_by_target\": r2s,\n",
    "        \"RMSE_by_target\": rmses,\n",
    "        \"MAPE_by_target(%)\": mapes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2210221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model, model_name, mode, save_dir=RESULTS_DIR):\n",
    "    \"\"\"í•™ìŠµ ê³¡ì„  ì‹œê°í™” (XGBoost, HistGBR)\"\"\"\n",
    "    \n",
    "    # XGBoost\n",
    "    if 'XGB' in model_name:\n",
    "        # GridSearchCVê°€ ì•„ë‹Œ ìµœì¢… ëª¨ë¸ í™•ì¸\n",
    "        if isinstance(model, xgb.XGBRegressor):\n",
    "            estimator = model\n",
    "        elif isinstance(model, MultiOutputRegressor):\n",
    "            estimator = model.estimators_[0]\n",
    "        elif isinstance(model, GridSearchCV):\n",
    "            estimator = model.best_estimator_\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        if hasattr(estimator, 'evals_result'):\n",
    "            results = estimator.evals_result()\n",
    "            if results and 'validation_0' in results:\n",
    "                train_metric = results['validation_0']['rmse']\n",
    "                valid_metric = results['validation_1']['rmse'] if 'validation_1' in results else None\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(train_metric, label='Train RMSE', linewidth=2)\n",
    "                if valid_metric:\n",
    "                    plt.plot(valid_metric, label='Valid RMSE', linewidth=2)\n",
    "                    # Best iteration í‘œì‹œ\n",
    "                    if hasattr(estimator, 'best_iteration'):\n",
    "                        plt.axvline(x=estimator.best_iteration, color='r', linestyle='--', \n",
    "                                   label=f'Best Iteration ({estimator.best_iteration})', alpha=0.7)\n",
    "                \n",
    "                plt.xlabel('Iterations', fontsize=12)\n",
    "                plt.ylabel('RMSE', fontsize=12)\n",
    "                plt.title(f'{model_name} Learning Curve - {mode.upper()}', fontsize=14, fontweight='bold')\n",
    "                plt.legend(fontsize=11)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                save_path = os.path.join(save_dir, f'{mode}_{model_name}_learning_curve.png')\n",
    "                plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "                print(f\"  ğŸ“Š Learning curve saved: {save_path}\")\n",
    "                plt.close()\n",
    "                return True\n",
    "    \n",
    "    # HistGradientBoosting\n",
    "    elif 'HistGBR' in model_name:\n",
    "        if isinstance(model, MultiOutputRegressor):\n",
    "            estimator = model.estimators_[0]\n",
    "        elif isinstance(model, GridSearchCV):\n",
    "            estimator = model.best_estimator_\n",
    "        else:\n",
    "            estimator = model\n",
    "        \n",
    "        # HistGBRì€ train_score_ ì†ì„±ì´ ìˆìŒ\n",
    "        if hasattr(estimator, 'train_score_'):\n",
    "            train_scores = estimator.train_score_\n",
    "            valid_scores = estimator.validation_score_ if hasattr(estimator, 'validation_score_') else None\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_scores, label='Train Score', linewidth=2)\n",
    "            if valid_scores is not None:\n",
    "                plt.plot(valid_scores, label='Valid Score', linewidth=2)\n",
    "            plt.xlabel('Iterations', fontsize=12)\n",
    "            plt.ylabel('Score', fontsize=12)\n",
    "            plt.title(f'{model_name} Learning Curve - {mode.upper()}', fontsize=14, fontweight='bold')\n",
    "            plt.legend(fontsize=11)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            save_path = os.path.join(save_dir, f'{mode}_{model_name}_learning_curve.png')\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"  ğŸ“Š Learning curve saved: {save_path}\")\n",
    "            plt.close()\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d060a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_r2_comparison(results, mode, save_dir=RESULTS_DIR):\n",
    "    \"\"\"ëª¨ë“  ëª¨ë¸ì˜ RÂ² ë¹„êµ ì‹œê°í™”\"\"\"\n",
    "    models = list(results.keys())\n",
    "    train_r2 = [results[m]['train']['R2_mean'] for m in models]\n",
    "    valid_r2 = [results[m]['valid']['R2_mean'] for m in models]\n",
    "    test_r2 = [results[m]['test']['R2_mean'] for m in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width, train_r2, width, label='Train', alpha=0.8)\n",
    "    ax.bar(x, valid_r2, width, label='Valid', alpha=0.8)\n",
    "    ax.bar(x + width, test_r2, width, label='Test', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Models', fontsize=12)\n",
    "    ax.set_ylabel('RÂ² Score', fontsize=12)\n",
    "    ax.set_title(f'RÂ² Comparison - {mode.upper()}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.axhline(y=0, color='r', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f'{mode}_r2_comparison.png')\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nğŸ“Š RÂ² comparison saved: {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(model, splits, scaler=None, model_name=\"\", mode=\"\", target_cols=None):\n",
    "    \"\"\"StandardScaler ì ìš© ë° í‰ê°€ (ê° íƒ€ê²Ÿë³„ ê°œë³„ í•™ìŠµ)\"\"\"\n",
    "    X_train, y_train = splits[\"train\"]\n",
    "    X_valid, y_valid = splits[\"valid\"]\n",
    "    X_test, y_test = splits[\"test\"]\n",
    "    \n",
    "    # Scaling\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # ë‹¤ì¤‘ íƒ€ê²Ÿì¸ ê²½ìš° ê°ê° ê°œë³„ í•™ìŠµ\n",
    "    n_targets = y_train.shape[1] if len(y_train.shape) > 1 else 1\n",
    "    \n",
    "    if n_targets > 1:\n",
    "        print(f\"  Training {n_targets} individual models for each target...\")\n",
    "        fitted_models = {}\n",
    "        all_results = {\"train\": [], \"valid\": [], \"test\": []}\n",
    "        \n",
    "        for i, target_name in enumerate(target_cols):\n",
    "            print(f\"\\n  [{i+1}/{n_targets}] Training for {target_name}...\")\n",
    "            \n",
    "            y_train_single = y_train.iloc[:, i] if hasattr(y_train, 'iloc') else y_train[:, i]\n",
    "            y_valid_single = y_valid.iloc[:, i] if hasattr(y_valid, 'iloc') else y_valid[:, i]\n",
    "            y_test_single = y_test.iloc[:, i] if hasattr(y_test, 'iloc') else y_test[:, i]\n",
    "            \n",
    "            # ëª¨ë¸ ë³µì‚¬ (GridSearchCVëŠ” ë³µì‚¬ ë¶ˆê°€í•˜ë¯€ë¡œ ì¬ìƒì„±)\n",
    "            if isinstance(model, GridSearchCV):\n",
    "                single_model = type(model)(\n",
    "                    estimator=type(model.estimator)(**model.estimator.get_params()),\n",
    "                    param_grid=model.param_grid,\n",
    "                    cv=model.cv,\n",
    "                    scoring=model.scoring,\n",
    "                    n_jobs=model.n_jobs,\n",
    "                    verbose=0\n",
    "                )\n",
    "            else:\n",
    "                single_model = type(model)(**model.get_params())\n",
    "            \n",
    "            # í•™ìŠµ\n",
    "            if isinstance(single_model, GridSearchCV):\n",
    "                single_model.fit(X_train_scaled, y_train_single)\n",
    "                print(f\"    Best params: {single_model.best_params_}\")\n",
    "                \n",
    "                # XGBoost Early Stopping\n",
    "                if 'XGB' in model_name:\n",
    "                    best_params = single_model.best_params_.copy()\n",
    "                    best_params['n_estimators'] = 500  # ë®ì–´ì“°ê¸°\n",
    "                    \n",
    "                    final_model = xgb.XGBRegressor(\n",
    "                        **best_params,\n",
    "                        early_stopping_rounds=20,\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "                    \n",
    "                    final_model.fit(\n",
    "                        X_train_scaled, y_train_single,\n",
    "                        eval_set=[(X_train_scaled, y_train_single), (X_valid_scaled, y_valid_single)],\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    \n",
    "                    if hasattr(final_model, 'best_iteration'):\n",
    "                        print(f\"    Early stopping at iteration: {final_model.best_iteration}\")\n",
    "                    single_model = final_model\n",
    "                \n",
    "                elif 'HistGBR' in model_name:\n",
    "                    if hasattr(single_model.best_estimator_, 'n_iter_'):\n",
    "                        print(f\"    Iterations: {single_model.best_estimator_.n_iter_}\")\n",
    "            else:\n",
    "                single_model.fit(X_train_scaled, y_train_single)\n",
    "            \n",
    "            fitted_models[target_name] = single_model\n",
    "            \n",
    "            # ì˜ˆì¸¡\n",
    "            pred_train = single_model.predict(X_train_scaled)\n",
    "            pred_valid = single_model.predict(X_valid_scaled)\n",
    "            pred_test = single_model.predict(X_test_scaled)\n",
    "            \n",
    "            # ë©”íŠ¸ë¦­ ê³„ì‚° (ë‹¨ì¼ íƒ€ê²Ÿ)\n",
    "            all_results[\"train\"].append(compute_metrics(y_train_single, pred_train))\n",
    "            all_results[\"valid\"].append(compute_metrics(y_valid_single, pred_valid))\n",
    "            all_results[\"test\"].append(compute_metrics(y_test_single, pred_test))\n",
    "        \n",
    "        # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        out = {}\n",
    "        for split in [\"train\", \"valid\", \"test\"]:\n",
    "            out[split] = {\n",
    "                \"R2_mean\": float(np.mean([r[\"R2_mean\"] for r in all_results[split]])),\n",
    "                \"RMSE_mean\": float(np.mean([r[\"RMSE_mean\"] for r in all_results[split]])),\n",
    "                \"MAPE_mean(%)\": float(np.mean([r[\"MAPE_mean(%)\"] for r in all_results[split]])),\n",
    "                \"R2_by_target\": [r[\"R2_mean\"] for r in all_results[split]],\n",
    "                \"RMSE_by_target\": [r[\"RMSE_mean\"] for r in all_results[split]],\n",
    "                \"MAPE_by_target(%)\": [r[\"MAPE_mean(%)\"] for r in all_results[split]],\n",
    "            }\n",
    "        \n",
    "        # Learning curveëŠ” ì²« ë²ˆì§¸ íƒ€ê²Ÿ ëª¨ë¸ë§Œ ì‹œê°í™”\n",
    "        first_model = fitted_models[target_cols[0]]\n",
    "        plot_learning_curve(first_model, model_name, mode)\n",
    "        \n",
    "        return out, scaler, fitted_models\n",
    "    \n",
    "    else:\n",
    "        # ë‹¨ì¼ íƒ€ê²Ÿ\n",
    "        print(f\"  Training single target model...\")\n",
    "        \n",
    "        # GridSearchCVë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "        if isinstance(model, GridSearchCV):\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            print(f\"  Best params: {model.best_params_}\")\n",
    "            \n",
    "            # XGBoostì¸ ê²½ìš° ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì¬í•™ìŠµ (Early Stopping ì ìš©)\n",
    "            if 'XGB' in model_name:\n",
    "                best_params = model.best_params_.copy()\n",
    "                best_params['n_estimators'] = 500  # ë®ì–´ì“°ê¸°\n",
    "                \n",
    "                # XGBoost 3.x: early_stopping_roundsëŠ” ìƒì„±ì íŒŒë¼ë¯¸í„°\n",
    "                final_model = xgb.XGBRegressor(\n",
    "                    **best_params,\n",
    "                    early_stopping_rounds=20,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                final_model.fit(\n",
    "                    X_train_scaled, y_train,\n",
    "                    eval_set=[(X_train_scaled, y_train), (X_valid_scaled, y_valid)],\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                if hasattr(final_model, 'best_iteration'):\n",
    "                    print(f\"  Early stopping at iteration: {final_model.best_iteration}\")\n",
    "                model = final_model  # GridSearchCV ëª¨ë¸ì„ ìµœì¢… ëª¨ë¸ë¡œ êµì²´\n",
    "            \n",
    "            # HistGBRì€ ì´ë¯¸ early_stoppingì´ íŒŒë¼ë¯¸í„°ì— í¬í•¨ë¨\n",
    "            elif 'HistGBR' in model_name:\n",
    "                if hasattr(model.best_estimator_, 'n_iter_'):\n",
    "                    print(f\"  Iterations: {model.best_estimator_.n_iter_}\")\n",
    "        \n",
    "        # ì¼ë°˜ ëª¨ë¸\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluation\n",
    "        out = {}\n",
    "        pred_train = model.predict(X_train_scaled)\n",
    "        pred_valid = model.predict(X_valid_scaled)\n",
    "        pred_test = model.predict(X_test_scaled)\n",
    "        \n",
    "        out[\"train\"] = compute_metrics(y_train.to_numpy(), pred_train)\n",
    "        out[\"valid\"] = compute_metrics(y_valid.to_numpy(), pred_valid)\n",
    "        out[\"test\"] = compute_metrics(y_test.to_numpy(), pred_test)\n",
    "        \n",
    "        # Learning curve ì‹œê°í™”\n",
    "        plot_learning_curve(model, model_name, mode)\n",
    "        \n",
    "        return out, scaler, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b18bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_table(result_by_model, split=\"test\"):\n",
    "    rows = []\n",
    "    for model_name, res in result_by_model.items():\n",
    "        m = res[split]\n",
    "        rows.append([model_name, m[\"R2_mean\"], m[\"RMSE_mean\"], m[\"MAPE_mean(%)\"]])\n",
    "    tbl = pd.DataFrame(rows, columns=[\"model\", \"R2_mean\", \"RMSE_mean\", \"MAPE_mean(%)\"])\n",
    "    return tbl.sort_values(by=\"R2_mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33128a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8) Pipeline Runner\n",
    "# =========================\n",
    "def run_improved_pipeline(\n",
    "    dfs,\n",
    "    mode,\n",
    "    time_col_map=None,\n",
    "    tz=None,\n",
    "    resample_rule=\"1h\",\n",
    "    resample_agg=\"mean\",\n",
    "    feature_cfg=FeatureConfig(),\n",
    "    split_cfg=SplitConfig(),\n",
    "    n_top_features=50,\n",
    "    cv_splits=3,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"ê°œì„ ëœ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Mode: {mode.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    target_cols = get_target_cols(mode)\n",
    "    \n",
    "    # 1) ë°ì´í„° ì¸ë±ì‹±\n",
    "    dfs_indexed = {}\n",
    "    for name, df in dfs.items():\n",
    "        if df is None or len(df) == 0:\n",
    "            dfs_indexed[name] = df\n",
    "            continue\n",
    "        \n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            dfs_indexed[name] = df.sort_index()\n",
    "        else:\n",
    "            if time_col_map is None or name not in time_col_map:\n",
    "                raise ValueError(f\"{name} has no DatetimeIndex and no time_col_map provided.\")\n",
    "            dfs_indexed[name] = set_datetime_index(df, time_col=time_col_map[name], tz=tz)\n",
    "    \n",
    "    # 2) ë³‘í•©\n",
    "    df_all = merge_sources_on_time(dfs_indexed, how=\"outer\")\n",
    "    \n",
    "    # 3) ë¦¬ìƒ˜í”Œë§\n",
    "    df_hourly = resample_hourly(df_all, rule=resample_rule, agg=resample_agg)\n",
    "    \n",
    "    # 4) í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "    df_feat = build_features(\n",
    "        df_hourly=df_hourly,\n",
    "        target_cols=target_cols,\n",
    "        feature_base_cols=None,\n",
    "        cfg=feature_cfg\n",
    "    )\n",
    "    \n",
    "    # 5) ì§€ë„í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± (ê²°ì¸¡ì¹˜ ì œê±°)\n",
    "    X, y = make_supervised_dataset(df_feat, target_cols=target_cols)\n",
    "    \n",
    "    # 6) ë¶„í• \n",
    "    splits = time_split(X, y, cfg=split_cfg)\n",
    "    X_train, y_train = splits[\"train\"]\n",
    "    \n",
    "    # 7) í”¼ì²˜ ì„ íƒ\n",
    "    top_features = select_top_features(X_train, y_train, n_features=n_top_features)\n",
    "    X_train_selected = X_train[top_features]\n",
    "    X_valid_selected = splits[\"valid\"][0][top_features]\n",
    "    X_test_selected = splits[\"test\"][0][top_features]\n",
    "    \n",
    "    splits_selected = {\n",
    "        \"train\": (X_train_selected, y_train),\n",
    "        \"valid\": (X_valid_selected, splits[\"valid\"][1]),\n",
    "        \"test\": (X_test_selected, splits[\"test\"][1])\n",
    "    }\n",
    "    \n",
    "    # 8) ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
    "    zoo = build_model_zoo_with_gridsearch(n_targets=y_train.shape[1], cv=cv_splits)\n",
    "    \n",
    "    results = {}\n",
    "    fitted_models = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    for model_name, base_model in zoo.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training: {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        model = wrap_multioutput_if_needed(base_model, y_train)\n",
    "        res, scaler, fitted_model = fit_and_evaluate(\n",
    "            model, splits_selected, \n",
    "            model_name=model_name, \n",
    "            mode=mode,\n",
    "            target_cols=target_cols\n",
    "        )\n",
    "        \n",
    "        results[model_name] = res\n",
    "        fitted_models[model_name] = fitted_model\n",
    "        scalers[model_name] = scaler\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"\\n  Train - RÂ²: {res['train']['R2_mean']:.4f}, RMSE: {res['train']['RMSE_mean']:.2f}\")\n",
    "        print(f\"  Valid - RÂ²: {res['valid']['R2_mean']:.4f}, RMSE: {res['valid']['RMSE_mean']:.2f}\")\n",
    "        print(f\"  Test  - RÂ²: {res['test']['R2_mean']:.4f}, RMSE: {res['test']['RMSE_mean']:.2f}\")\n",
    "        \n",
    "        # íƒ€ê²Ÿë³„ ì„±ëŠ¥ ì¶œë ¥\n",
    "        if len(target_cols) > 1:\n",
    "            print(f\"\\n  Per-target Test RÂ²:\")\n",
    "            for i, target_name in enumerate(target_cols):\n",
    "                print(f\"    {target_name}: {res['test']['R2_by_target'][i]:.4f}\")\n",
    "        \n",
    "        # Overfitting ì²´í¬\n",
    "        train_r2 = res['train']['R2_mean']\n",
    "        valid_r2 = res['valid']['R2_mean']\n",
    "        if train_r2 - valid_r2 > 0.1:\n",
    "            print(f\"  âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„±: Train RÂ² ({train_r2:.4f}) >> Valid RÂ² ({valid_r2:.4f})\")\n",
    "    \n",
    "    metric_table = plot_metric_table(results, split=\"test\")\n",
    "    \n",
    "    # RÂ² ë¹„êµ ì‹œê°í™”\n",
    "    plot_r2_comparison(results, mode)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ìµœì¢… ê²°ê³¼ (Test Set)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(metric_table.to_string(index=False))\n",
    "    \n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"target_cols\": target_cols,\n",
    "        \"df_hourly\": df_hourly,\n",
    "        \"df_features\": df_feat,\n",
    "        \"X\": X, \"y\": y,\n",
    "        \"splits\": splits_selected,\n",
    "        \"top_features\": top_features,\n",
    "        \"results\": results,\n",
    "        \"metric_table\": metric_table,\n",
    "        \"fitted_models\": fitted_models,\n",
    "        \"scalers\": scalers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e384f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 9) Main Execution\n",
    "# =========================\n",
    "\n",
    "print(\"ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df_flow = pd.read_csv(\"../../../../data/actual/FLOW_Actual.csv\")\n",
    "df_tms = pd.read_csv(\"../../../../data/actual/TMS_Actual.csv\")\n",
    "df_aws_368 = pd.read_csv(\"../../../../data/actual/AWS_368.csv\")\n",
    "df_aws_541 = pd.read_csv(\"../../../../data/actual/AWS_541.csv\")\n",
    "df_aws_569 = pd.read_csv(\"../../../../data/actual/AWS_569.csv\")\n",
    "\n",
    "print(f\"FLOW columns: {df_flow.columns.tolist()}\")\n",
    "print(f\"TMS columns: {df_tms.columns.tolist()}\")\n",
    "print(f\"AWS columns: {df_aws_368.columns.tolist()}\")\n",
    "\n",
    "# Q_in ì»¬ëŸ¼ ìƒì„±: Q_in = flow_TankA + flow_TankB\n",
    "if 'Q_in' not in df_flow.columns:\n",
    "    if 'flow_TankA' in df_flow.columns and 'flow_TankB' in df_flow.columns:\n",
    "        df_flow['Q_in'] = df_flow['flow_TankA'] + df_flow['flow_TankB']\n",
    "        print(\"Q_in ì»¬ëŸ¼ ìƒì„±: flow_TankA + flow_TankB\")\n",
    "    else:\n",
    "        print(\"WARNING: Q_in ì»¬ëŸ¼ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. FLOW ì˜ˆì¸¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "\n",
    "# âš ï¸ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€: flow_TankA, flow_TankB ì œê±° (Q_inì˜ êµ¬ì„± ìš”ì†Œ)\n",
    "# level_TankA, level_TankBëŠ” ìœ ì§€ (ë…ë¦½ ë³€ìˆ˜ë¡œ ì‚¬ìš© ê°€ëŠ¥)\n",
    "for c in [\"flow_TankA\", \"flow_TankB\"]:\n",
    "    if c in df_flow.columns:\n",
    "        df_flow = df_flow.drop(columns=[c])\n",
    "        print(f\"  Dropped {c} to prevent data leakage\")\n",
    "\n",
    "# AWS ë°ì´í„° ë³‘í•© (datetime ì»¬ëŸ¼ ì‚¬ìš©)\n",
    "df_aws = df_aws_368.copy()\n",
    "for df in [df_aws_541, df_aws_569]:\n",
    "    df_aws = df_aws.merge(df, on=\"datetime\", how=\"outer\", suffixes=(\"\", \"_dup\"))\n",
    "    df_aws = df_aws[[c for c in df_aws.columns if not c.endswith(\"_dup\")]]\n",
    "\n",
    "# ì‹œê°„ ì»¬ëŸ¼ëª… í†µì¼\n",
    "df_flow = df_flow.rename(columns={'SYS_TIME': 'time'})\n",
    "df_tms = df_tms.rename(columns={'SYS_TIME': 'time'})\n",
    "df_aws = df_aws.rename(columns={'datetime': 'time'})\n",
    "\n",
    "dfs = {\n",
    "    \"flow\": df_flow,\n",
    "    \"tms\": df_tms,\n",
    "    \"aws\": df_aws\n",
    "}\n",
    "\n",
    "time_col_map = {\n",
    "    \"flow\": \"time\",\n",
    "    \"tms\": \"time\",\n",
    "    \"aws\": \"time\"\n",
    "}\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVED ML BASELINE - WWTP PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Flow ì˜ˆì¸¡\n",
    "out_flow = run_improved_pipeline(\n",
    "    dfs, \n",
    "    mode=\"flow\", \n",
    "    time_col_map=time_col_map,\n",
    "    n_top_features=30,\n",
    "    cv_splits=3\n",
    ")\n",
    "\n",
    "# TMS ì˜ˆì¸¡\n",
    "out_tms = run_improved_pipeline(\n",
    "    dfs, \n",
    "    mode=\"tms\", \n",
    "    time_col_map=time_col_map,\n",
    "    n_top_features=40,\n",
    "    cv_splits=3\n",
    ")\n",
    "\n",
    "# All ì˜ˆì¸¡\n",
    "out_all = run_improved_pipeline(\n",
    "    dfs, \n",
    "    mode=\"all\", \n",
    "    time_col_map=time_col_map,\n",
    "    n_top_features=50,\n",
    "    cv_splits=3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ìµœê³  ëª¨ë¸ ì €ì¥ (ì„ íƒì‚¬í•­)\n",
    "best_model_name = out_all[\"metric_table\"].iloc[0][\"model\"]\n",
    "print(f\"\\nìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}\")\n",
    "print(f\"Test RÂ²: {out_all['results'][best_model_name]['test']['R2_mean']:.4f}\")\n",
    "print(f\"Test RMSE: {out_all['results'][best_model_name]['test']['RMSE_mean']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
