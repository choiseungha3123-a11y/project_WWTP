{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a0b553",
   "metadata": {},
   "source": [
    "Improved ML Baseline V2 for WWTP Prediction\n",
    "\n",
    "ì£¼ìš” ê°œì„ ì‚¬í•­:\n",
    "\n",
    "1. ê²°ì¸¡ì¹˜ ë³´ê°„ (ì„ í˜•, KNN, Forward Fill)\n",
    "2. ë„ë©”ì¸ í”¼ì²˜ ì¶”ê°€ (ìƒí˜¸ì‘ìš©, ë¹„ìœ¨, ì°¨ë¶„)\n",
    "3. ì •ê·œí™” ê°•í™” (alpha ì¦ê°€, max_depth ì œí•œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # ë°±ê·¸ë¼ìš´ë“œ ëª¨ë“œ (GUI ì—†ìŒ)\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "RESULTS_DIR = \"../../../../results/ML/v2\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bc282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Target config\n",
    "# =========================\n",
    "TARGETS_FLOW = [\"Q_in\"]\n",
    "TARGETS_TMS = [\"TOC_VU\", \"PH_VU\", \"SS_VU\", \"FLUX_VU\", \"TN_VU\", \"TP_VU\"]\n",
    "TARGETS_ALL = TARGETS_FLOW + TARGETS_TMS\n",
    "\n",
    "def get_target_cols(mode):\n",
    "    mode = mode.lower().strip()\n",
    "    if mode == \"flow\":\n",
    "        return TARGETS_FLOW\n",
    "    if mode == \"tms\":\n",
    "        return TARGETS_TMS\n",
    "    if mode == \"all\":\n",
    "        return TARGETS_ALL\n",
    "    raise ValueError(\"mode must be one of: 'flow', 'tms', 'all'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a458ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Time index & merge\n",
    "# =========================\n",
    "def set_datetime_index(df, time_col, tz=None):\n",
    "    out = df.copy()\n",
    "    out[time_col] = pd.to_datetime(out[time_col], errors=\"coerce\")\n",
    "    out = out.dropna(subset=[time_col]).set_index(time_col).sort_index()\n",
    "    return out\n",
    "\n",
    "def merge_sources_on_time(dfs, how=\"outer\"):\n",
    "    items = [df.copy() for df in dfs.values() if df is not None and len(df) > 0]\n",
    "    if not items:\n",
    "        raise ValueError(\"No non-empty dataframes to merge.\")\n",
    "    # ì „ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” ì´ë¯¸ ë³‘í•©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ë§Œ ë°˜í™˜\n",
    "    if len(items) == 1:\n",
    "        return items[0].sort_index()\n",
    "    out = items[0]\n",
    "    for nxt in items[1:]:\n",
    "        out = out.join(nxt, how=how)\n",
    "    out = out.sort_index()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) ë¦¬ìƒ˜í”Œë§\n",
    "# =========================\n",
    "def resample_hourly(df, rule=\"1h\", agg=\"mean\"):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"df must have a DatetimeIndex for resampling.\")\n",
    "    out = df.copy()\n",
    "    if isinstance(agg, str):\n",
    "        return out.resample(rule).agg(agg)\n",
    "    return out.resample(rule).agg(agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd924d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) ë„ë©”ì¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ê°œì„ !)\n",
    "# =========================\n",
    "def add_time_features(df, add_sin_cos=True):\n",
    "    out = df.copy()\n",
    "    idx = out.index\n",
    "    if not isinstance(idx, pd.DatetimeIndex):\n",
    "        raise ValueError(\"df must have a DatetimeIndex for time features.\")\n",
    "    \n",
    "    out[\"hour\"] = idx.hour\n",
    "    out[\"dayofweek\"] = idx.dayofweek\n",
    "    out[\"month\"] = idx.month\n",
    "    out[\"is_weekend\"] = (idx.dayofweek >= 5).astype(int)\n",
    "    out[\"is_night\"] = ((idx.hour >= 22) | (idx.hour <= 6)).astype(int)\n",
    "    \n",
    "    # Season\n",
    "    m = out[\"month\"]\n",
    "    season_map = {12:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3}\n",
    "    out[\"season\"] = m.map(season_map).astype(int)\n",
    "    \n",
    "    if add_sin_cos:\n",
    "        out[\"sin_hour\"] = np.sin(2 * np.pi * out[\"hour\"] / 24.0)\n",
    "        out[\"cos_hour\"] = np.cos(2 * np.pi * out[\"hour\"] / 24.0)\n",
    "        out[\"sin_dow\"] = np.sin(2 * np.pi * out[\"dayofweek\"] / 7.0)\n",
    "        out[\"cos_dow\"] = np.cos(2 * np.pi * out[\"dayofweek\"] / 7.0)\n",
    "        out[\"sin_month\"] = np.sin(2 * np.pi * out[\"month\"] / 12.0)\n",
    "        out[\"cos_month\"] = np.cos(2 * np.pi * out[\"month\"] / 12.0)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_domain_features(df, target_cols):\n",
    "    \"\"\"ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í”¼ì²˜ ì¶”ê°€\"\"\"\n",
    "    out = df.copy()\n",
    "    print(\"\\në„ë©”ì¸ í”¼ì²˜ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    # 1. ê°•ìˆ˜ëŸ‰ ê´€ë ¨ í”¼ì²˜\n",
    "    if 'RN_60m' in out.columns and 'RN_DAY' in out.columns:\n",
    "        out['rain_intensity'] = out['RN_60m'] / (out['RN_DAY'] + 1)  # ê°•ìˆ˜ ê°•ë„\n",
    "        out['is_raining'] = (out['RN_60m'] > 0).astype(int)\n",
    "        print(\"  âœ“ ê°•ìˆ˜ëŸ‰ í”¼ì²˜ ì¶”ê°€\")\n",
    "    \n",
    "    # 2. ì˜¨ë„-ìŠµë„ ê´€ë ¨ í”¼ì²˜\n",
    "    if 'TA' in out.columns and 'HM' in out.columns:\n",
    "        out['temp_humidity_idx'] = out['TA'] * out['HM'] / 100  # ë¶ˆì¾Œì§€ìˆ˜ ê·¼ì‚¬\n",
    "        out['temp_range'] = out['TA'].rolling(24, min_periods=1).max() - out['TA'].rolling(24, min_periods=1).min()\n",
    "        print(\"  âœ“ ì˜¨ë„-ìŠµë„ í”¼ì²˜ ì¶”ê°€\")\n",
    "    \n",
    "    # 3. íƒ±í¬ ìˆ˜ìœ„ ê´€ë ¨ í”¼ì²˜\n",
    "    if 'level_TankA' in out.columns and 'level_TankB' in out.columns:\n",
    "        out['tank_total_level'] = out['level_TankA'] + out['level_TankB']\n",
    "        out['tank_level_diff'] = abs(out['level_TankA'] - out['level_TankB'])\n",
    "        out['tank_level_ratio'] = out['level_TankA'] / (out['level_TankB'] + 0.01)\n",
    "        print(\"  âœ“ íƒ±í¬ ìˆ˜ìœ„ í”¼ì²˜ ì¶”ê°€\")\n",
    "    \n",
    "    # 4. ìˆ˜ì§ˆ ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš© (TMS ì˜ˆì¸¡ ì‹œ)\n",
    "    if 'TOC_VU' in out.columns and 'TN_VU' in out.columns:\n",
    "        out['TOC_TN_ratio'] = out['TOC_VU'] / (out['TN_VU'] + 0.01)\n",
    "        print(\"  âœ“ TOC/TN ë¹„ìœ¨ ì¶”ê°€\")\n",
    "    \n",
    "    if 'SS_VU' in out.columns and 'TP_VU' in out.columns:\n",
    "        out['SS_TP_ratio'] = out['SS_VU'] / (out['TP_VU'] + 0.01)\n",
    "        print(\"  âœ“ SS/TP ë¹„ìœ¨ ì¶”ê°€\")\n",
    "    \n",
    "    # 5. ì°¨ë¶„ í”¼ì²˜ (ë³€í™”ìœ¨)\n",
    "    for col in ['TA', 'HM', 'level_TankA', 'level_TankB']:\n",
    "        if col in out.columns:\n",
    "            out[f'{col}_diff1'] = out[col].diff(1)  # 1ì‹œê°„ ë³€í™”\n",
    "            out[f'{col}_diff24'] = out[col].diff(24)  # 24ì‹œê°„ ë³€í™”\n",
    "    \n",
    "    print(f\"  ì´ {len(out.columns) - len(df.columns)}ê°œ ë„ë©”ì¸ í”¼ì²˜ ì¶”ê°€ë¨\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(df, base_cols, lags):\n",
    "    out = df.copy()\n",
    "    for c in base_cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        for k in lags:\n",
    "            out[f\"{c}_lag{k}\"] = out[c].shift(k)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00645589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(df, base_cols, windows, stats=[\"mean\"]):\n",
    "    out = df.copy()\n",
    "    for c in base_cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        for w in windows:\n",
    "            r = out[c].rolling(window=w, min_periods=max(1, w//2))  # min_periods ì™„í™”\n",
    "            if \"mean\" in stats:\n",
    "                out[f\"{c}_r{w}_mean\"] = r.mean()\n",
    "            if \"std\" in stats:\n",
    "                out[f\"{c}_r{w}_std\"] = r.std()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eeed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    add_time = True\n",
    "    add_sin_cos = True\n",
    "    add_domain = True  # ë„ë©”ì¸ í”¼ì²˜ ì¶”ê°€\n",
    "    lag_hours = None\n",
    "    roll_hours = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.lag_hours is None:\n",
    "            self.lag_hours = [1, 3, 6, 12]  # 24 ì œê±° (ê³¼ì í•© ë°©ì§€)\n",
    "        if self.roll_hours is None:\n",
    "            self.roll_hours = [3, 6, 12]  # ë” ì§§ì€ ìœˆë„ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b897c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(df_hourly, target_cols, feature_base_cols=None, cfg=FeatureConfig()):\n",
    "    out = df_hourly.copy()\n",
    "    \n",
    "    if cfg.add_time:\n",
    "        out = add_time_features(out, add_sin_cos=cfg.add_sin_cos)\n",
    "    \n",
    "    if cfg.add_domain:\n",
    "        out = add_domain_features(out, target_cols)\n",
    "    \n",
    "    if feature_base_cols is None:\n",
    "        numeric_cols = out.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        feature_base_cols = [c for c in numeric_cols if c not in target_cols]\n",
    "    \n",
    "    out = add_lag_features(out, base_cols=feature_base_cols, lags=cfg.lag_hours)\n",
    "    out = add_rolling_features(out, base_cols=feature_base_cols, windows=cfg.roll_hours)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87289c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised_dataset(df, target_cols):\n",
    "    \"\"\"ê²°ì¸¡ì¹˜ ì œê±° (ë³´ê°„ í›„ì´ë¯€ë¡œ ìµœì†Œí™”ë¨)\"\"\"\n",
    "    missing = [c for c in target_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"target_cols not found in df: {missing}\")\n",
    "    \n",
    "    y = df[target_cols].copy()\n",
    "    X = df.drop(columns=target_cols).copy()\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ì œê±°\n",
    "    keep = X.notna().all(axis=1) & y.notna().all(axis=1)\n",
    "    X_clean = X.loc[keep]\n",
    "    y_clean = y.loc[keep]\n",
    "    \n",
    "    print(f\"\\nìµœì¢… ë°ì´í„°ì…‹: {len(X)} â†’ {len(X_clean)} ({len(X_clean)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    return X_clean, y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Split (time-based)\n",
    "# =========================\n",
    "@dataclass\n",
    "class SplitConfig:\n",
    "    train_ratio = 0.6\n",
    "    valid_ratio = 0.2\n",
    "    test_ratio = 0.2\n",
    "\n",
    "def time_split(X, y, cfg=SplitConfig()):\n",
    "    n = len(X)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Empty dataset after preprocessing/feature generation.\")\n",
    "    \n",
    "    n_train = int(n * cfg.train_ratio)\n",
    "    n_valid = int(n * cfg.valid_ratio)\n",
    "    \n",
    "    X_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\n",
    "    X_valid, y_valid = X.iloc[n_train:n_train+n_valid], y.iloc[n_train:n_train+n_valid]\n",
    "    X_test, y_test = X.iloc[n_train+n_valid:], y.iloc[n_train+n_valid:]\n",
    "    \n",
    "    return {\n",
    "        \"train\": (X_train, y_train),\n",
    "        \"valid\": (X_valid, y_valid),\n",
    "        \"test\": (X_test, y_test),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5) Feature Selection\n",
    "# =========================\n",
    "def select_top_features(X_train, y_train, n_features=30):\n",
    "    \"\"\"RandomForestë¡œ í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚° í›„ ìƒìœ„ nê°œ ì„ íƒ (V2: ë” ì ì€ í”¼ì²˜)\"\"\"\n",
    "    print(f\"\\ní”¼ì²˜ ì„ íƒ ì¤‘... (ì´ {X_train.shape[1]}ê°œ â†’ ìƒìœ„ {n_features}ê°œ)\")\n",
    "    \n",
    "    # ë‹¨ì¼ íƒ€ê²Ÿì¸ ê²½ìš°\n",
    "    if len(y_train.shape) == 1 or y_train.shape[1] == 1:\n",
    "        rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train.values.ravel() if hasattr(y_train, 'values') else y_train)\n",
    "        importances = rf.feature_importances_\n",
    "    else:\n",
    "        # ë‹¤ì¤‘ íƒ€ê²Ÿì¸ ê²½ìš° í‰ê·  ì¤‘ìš”ë„ ì‚¬ìš©\n",
    "        rf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1))\n",
    "        rf.fit(X_train, y_train)\n",
    "        importances = np.mean([est.feature_importances_ for est in rf.estimators_], axis=0)\n",
    "    \n",
    "    # ìƒìœ„ nê°œ í”¼ì²˜ ì„ íƒ\n",
    "    top_indices = np.argsort(importances)[-n_features:]\n",
    "    top_features = X_train.columns[top_indices].tolist()\n",
    "    \n",
    "    print(f\"ì„ íƒëœ ìƒìœ„ 10ê°œ í”¼ì²˜: {top_features[-10:]}\")\n",
    "    \n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f86113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6) Models with GridSearch (V2: ê°•í™”ëœ ì •ê·œí™”)\n",
    "# =========================\n",
    "def build_model_zoo_with_gridsearch(n_targets=1, cv=3):\n",
    "    \"\"\"GridSearchCVë¥¼ í¬í•¨í•œ ëª¨ë¸ ì •ì˜ (V2: ì •ê·œí™” ê°•í™”)\"\"\"\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=cv)\n",
    "    \n",
    "    # Ridge (V2: alpha ì¦ê°€)\n",
    "    ridge_params = {\n",
    "        'alpha': [100, 1000, 10000]  # V1: [0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "    ridge = GridSearchCV(\n",
    "        Ridge(random_state=42),\n",
    "        ridge_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Lasso (V2: alpha ì¦ê°€)\n",
    "    lasso_params = {\n",
    "        'alpha': [1.0, 10.0, 100.0]  # V1: [0.001, 0.01, 0.1, 1.0]\n",
    "    }\n",
    "    lasso = GridSearchCV(\n",
    "        Lasso(random_state=42, max_iter=5000),\n",
    "        lasso_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # RandomForest (V2: max_depth ì œí•œ, min_samples_leaf ì¶”ê°€)\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [5, 10, 15],  # V1: [10, 20, None] - None ì œê±°\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [5, 10, 20]  # V2: ì¶”ê°€\n",
    "    }\n",
    "    rf = GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        rf_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    \n",
    "    # HistGradientBoosting (V2: ë” ê°•í•œ ì •ê·œí™”)\n",
    "    hgb_params = {\n",
    "        'learning_rate': [0.01, 0.05],  # V1: [0.01, 0.05, 0.1]\n",
    "        'max_iter': [500],\n",
    "        'max_depth': [5, 10],  # V1: [5, 10, 20]\n",
    "        'min_samples_leaf': [20, 30],  # V2: ì¶”ê°€\n",
    "        'early_stopping': [True],\n",
    "        'n_iter_no_change': [20],\n",
    "        'validation_fraction': [0.2]\n",
    "    }\n",
    "    hgb = GridSearchCV(\n",
    "        HistGradientBoostingRegressor(random_state=42),\n",
    "        hgb_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # XGBoost (V2: ì •ê·œí™” íŒŒë¼ë¯¸í„° ì¶”ê°€)\n",
    "    xgb_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05],  # V1: [0.01, 0.05, 0.1]\n",
    "        'max_depth': [3, 5],  # V1: [3, 5, 7]\n",
    "        'subsample': [0.8],  # V1: [0.8, 1.0]\n",
    "        'min_child_weight': [3, 5, 7],  # V2: ì¶”ê°€\n",
    "        'gamma': [0, 0.1, 0.2]  # V2: ì¶”ê°€\n",
    "    }\n",
    "    xgb_model = GridSearchCV(\n",
    "        xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "        xgb_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    zoo = {\n",
    "        \"Ridge\": ridge,\n",
    "        \"Lasso\": lasso,\n",
    "        \"RandomForest\": rf,\n",
    "        \"HistGBR\": hgb,\n",
    "        \"XGBoost\": xgb_model,\n",
    "    }\n",
    "    \n",
    "    return zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7) Metrics & Evaluation\n",
    "# =========================\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    yt = np.asarray(y_true)\n",
    "    yp = np.asarray(y_pred)\n",
    "    if yt.ndim == 1:\n",
    "        yt = yt.reshape(-1, 1)\n",
    "    if yp.ndim == 1:\n",
    "        yp = yp.reshape(-1, 1)\n",
    "    \n",
    "    r2s, rmses, mapes = [], [], []\n",
    "    for j in range(yt.shape[1]):\n",
    "        r2s.append(r2_score(yt[:, j], yp[:, j]))\n",
    "        rmses.append(math.sqrt(mean_squared_error(yt[:, j], yp[:, j])))\n",
    "        mapes.append(mean_absolute_percentage_error(yt[:, j], yp[:, j]) * 100.0)\n",
    "    \n",
    "    return {\n",
    "        \"R2_mean\": float(np.mean(r2s)),\n",
    "        \"RMSE_mean\": float(np.mean(rmses)),\n",
    "        \"MAPE_mean(%)\": float(np.mean(mapes)),\n",
    "        \"R2_by_target\": r2s,\n",
    "        \"RMSE_by_target\": rmses,\n",
    "        \"MAPE_by_target(%)\": mapes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model, model_name, mode, save_dir=RESULTS_DIR):\n",
    "    \"\"\"í•™ìŠµ ê³¡ì„  ì‹œê°í™” (XGBoost, HistGBR)\"\"\"\n",
    "    \n",
    "    # XGBoost\n",
    "    if 'XGB' in model_name:\n",
    "        if isinstance(model, xgb.XGBRegressor):\n",
    "            estimator = model\n",
    "        elif isinstance(model, MultiOutputRegressor):\n",
    "            estimator = model.estimators_[0]\n",
    "        elif isinstance(model, GridSearchCV):\n",
    "            estimator = model.best_estimator_\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        if hasattr(estimator, 'evals_result'):\n",
    "            results = estimator.evals_result()\n",
    "            if results and 'validation_0' in results:\n",
    "                train_metric = results['validation_0']['rmse']\n",
    "                valid_metric = results['validation_1']['rmse'] if 'validation_1' in results else None\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(train_metric, label='Train RMSE', linewidth=2)\n",
    "                if valid_metric:\n",
    "                    plt.plot(valid_metric, label='Valid RMSE', linewidth=2)\n",
    "                    if hasattr(estimator, 'best_iteration'):\n",
    "                        plt.axvline(x=estimator.best_iteration, color='r', linestyle='--', \n",
    "                                   label=f'Best Iteration ({estimator.best_iteration})', alpha=0.7)\n",
    "\n",
    "                \n",
    "                plt.xlabel('Iterations', fontsize=12)\n",
    "                plt.ylabel('RMSE', fontsize=12)\n",
    "                plt.title(f'{model_name} Learning Curve - {mode.upper()}', fontsize=14, fontweight='bold')\n",
    "                plt.legend(fontsize=11)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                save_path = os.path.join(save_dir, f'{mode}_{model_name}_learning_curve.png')\n",
    "                plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "                print(f\"  ğŸ“Š Learning curve saved: {save_path}\")\n",
    "                plt.close()\n",
    "                return True\n",
    "    \n",
    "    # HistGradientBoosting\n",
    "    elif 'HistGBR' in model_name:\n",
    "        if isinstance(model, MultiOutputRegressor):\n",
    "            estimator = model.estimators_[0]\n",
    "        elif isinstance(model, GridSearchCV):\n",
    "            estimator = model.best_estimator_\n",
    "        else:\n",
    "            estimator = model\n",
    "        \n",
    "        if hasattr(estimator, 'train_score_'):\n",
    "            train_scores = estimator.train_score_\n",
    "            valid_scores = estimator.validation_score_ if hasattr(estimator, 'validation_score_') else None\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_scores, label='Train Score', linewidth=2)\n",
    "            if valid_scores is not None:\n",
    "                plt.plot(valid_scores, label='Valid Score', linewidth=2)\n",
    "            plt.xlabel('Iterations', fontsize=12)\n",
    "            plt.ylabel('Score', fontsize=12)\n",
    "            plt.title(f'{model_name} Learning Curve - {mode.upper()}', fontsize=14, fontweight='bold')\n",
    "            plt.legend(fontsize=11)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            save_path = os.path.join(save_dir, f'{mode}_{model_name}_learning_curve.png')\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"  ğŸ“Š Learning curve saved: {save_path}\")\n",
    "            plt.close()\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47dabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_r2_comparison(results, mode, save_dir=RESULTS_DIR):\n",
    "    \"\"\"ëª¨ë“  ëª¨ë¸ì˜ RÂ² ë¹„êµ ì‹œê°í™”\"\"\"\n",
    "    models = list(results.keys())\n",
    "    train_r2 = [results[m]['train']['R2_mean'] for m in models]\n",
    "    valid_r2 = [results[m]['valid']['R2_mean'] for m in models]\n",
    "    test_r2 = [results[m]['test']['R2_mean'] for m in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width, train_r2, width, label='Train', alpha=0.8)\n",
    "    ax.bar(x, valid_r2, width, label='Valid', alpha=0.8)\n",
    "    ax.bar(x + width, test_r2, width, label='Test', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Models', fontsize=12)\n",
    "    ax.set_ylabel('RÂ² Score', fontsize=12)\n",
    "    ax.set_title(f'RÂ² Comparison - {mode.upper()} (V2)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.axhline(y=0, color='r', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f'{mode}_r2_comparison.png')\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nğŸ“Š RÂ² comparison saved: {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_table(result_by_model, split=\"test\"):\n",
    "    rows = []\n",
    "    for model_name, res in result_by_model.items():\n",
    "        m = res[split]\n",
    "        rows.append([model_name, m[\"R2_mean\"], m[\"RMSE_mean\"], m[\"MAPE_mean(%)\"]])\n",
    "    tbl = pd.DataFrame(rows, columns=[\"model\", \"R2_mean\", \"RMSE_mean\", \"MAPE_mean(%)\"])\n",
    "    return tbl.sort_values(by=\"R2_mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e16afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(model, splits, scaler=None, model_name=\"\", mode=\"\", target_cols=None):\n",
    "    \"\"\"StandardScaler ì ìš© ë° í‰ê°€ (ê° íƒ€ê²Ÿë³„ ê°œë³„ í•™ìŠµ)\"\"\"\n",
    "    X_train, y_train = splits[\"train\"]\n",
    "    X_valid, y_valid = splits[\"valid\"]\n",
    "    X_test, y_test = splits[\"test\"]\n",
    "    \n",
    "    # Scaling\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # ë‹¤ì¤‘ íƒ€ê²Ÿì¸ ê²½ìš° ê°ê° ê°œë³„ í•™ìŠµ\n",
    "    n_targets = y_train.shape[1] if len(y_train.shape) > 1 else 1\n",
    "    \n",
    "    if n_targets > 1:\n",
    "        print(f\"  Training {n_targets} individual models for each target...\")\n",
    "        fitted_models = {}\n",
    "        all_results = {\"train\": [], \"valid\": [], \"test\": []}\n",
    "        \n",
    "        for i, target_name in enumerate(target_cols):\n",
    "            print(f\"\\n  [{i+1}/{n_targets}] Training for {target_name}...\")\n",
    "            \n",
    "            y_train_single = y_train.iloc[:, i] if hasattr(y_train, 'iloc') else y_train[:, i]\n",
    "            y_valid_single = y_valid.iloc[:, i] if hasattr(y_valid, 'iloc') else y_valid[:, i]\n",
    "            y_test_single = y_test.iloc[:, i] if hasattr(y_test, 'iloc') else y_test[:, i]\n",
    "            \n",
    "            # ëª¨ë¸ ë³µì‚¬\n",
    "            if isinstance(model, GridSearchCV):\n",
    "                single_model = type(model)(\n",
    "                    estimator=type(model.estimator)(**model.estimator.get_params()),\n",
    "                    param_grid=model.param_grid,\n",
    "                    cv=model.cv,\n",
    "                    scoring=model.scoring,\n",
    "                    n_jobs=model.n_jobs,\n",
    "                    verbose=0\n",
    "                )\n",
    "            else:\n",
    "                single_model = type(model)(**model.get_params())\n",
    "            \n",
    "            # í•™ìŠµ\n",
    "            if isinstance(single_model, GridSearchCV):\n",
    "                single_model.fit(X_train_scaled, y_train_single)\n",
    "                print(f\"    Best params: {single_model.best_params_}\")\n",
    "                \n",
    "                # XGBoost Early Stopping\n",
    "                if 'XGB' in model_name:\n",
    "                    best_params = single_model.best_params_.copy()\n",
    "                    best_params['n_estimators'] = 500\n",
    "                    \n",
    "                    final_model = xgb.XGBRegressor(\n",
    "                        **best_params,\n",
    "                        early_stopping_rounds=20,\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "                    \n",
    "                    final_model.fit(\n",
    "                        X_train_scaled, y_train_single,\n",
    "                        eval_set=[(X_train_scaled, y_train_single), (X_valid_scaled, y_valid_single)],\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    \n",
    "                    if hasattr(final_model, 'best_iteration'):\n",
    "                        print(f\"    Early stopping at iteration: {final_model.best_iteration}\")\n",
    "                    single_model = final_model\n",
    "\n",
    "                \n",
    "                elif 'HistGBR' in model_name:\n",
    "                    if hasattr(single_model.best_estimator_, 'n_iter_'):\n",
    "                        print(f\"    Iterations: {single_model.best_estimator_.n_iter_}\")\n",
    "            else:\n",
    "                single_model.fit(X_train_scaled, y_train_single)\n",
    "            \n",
    "            fitted_models[target_name] = single_model\n",
    "            \n",
    "            # ì˜ˆì¸¡\n",
    "            pred_train = single_model.predict(X_train_scaled)\n",
    "            pred_valid = single_model.predict(X_valid_scaled)\n",
    "            pred_test = single_model.predict(X_test_scaled)\n",
    "            \n",
    "            # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "            all_results[\"train\"].append(compute_metrics(y_train_single, pred_train))\n",
    "            all_results[\"valid\"].append(compute_metrics(y_valid_single, pred_valid))\n",
    "            all_results[\"test\"].append(compute_metrics(y_test_single, pred_test))\n",
    "        \n",
    "        # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        out = {}\n",
    "        for split in [\"train\", \"valid\", \"test\"]:\n",
    "            out[split] = {\n",
    "                \"R2_mean\": float(np.mean([r[\"R2_mean\"] for r in all_results[split]])),\n",
    "                \"RMSE_mean\": float(np.mean([r[\"RMSE_mean\"] for r in all_results[split]])),\n",
    "                \"MAPE_mean(%)\": float(np.mean([r[\"MAPE_mean(%)\"] for r in all_results[split]])),\n",
    "                \"R2_by_target\": [r[\"R2_mean\"] for r in all_results[split]],\n",
    "                \"RMSE_by_target\": [r[\"RMSE_mean\"] for r in all_results[split]],\n",
    "                \"MAPE_by_target(%)\": [r[\"MAPE_mean(%)\"] for r in all_results[split]],\n",
    "            }\n",
    "        \n",
    "        # Learning curveëŠ” ì²« ë²ˆì§¸ íƒ€ê²Ÿ ëª¨ë¸ë§Œ ì‹œê°í™”\n",
    "        first_model = fitted_models[target_cols[0]]\n",
    "        plot_learning_curve(first_model, model_name, mode)\n",
    "        \n",
    "        return out, scaler, fitted_models\n",
    "    \n",
    "    else:\n",
    "        # ë‹¨ì¼ íƒ€ê²Ÿ\n",
    "        print(f\"  Training single target model...\")\n",
    "        \n",
    "        if isinstance(model, GridSearchCV):\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            print(f\"  Best params: {model.best_params_}\")\n",
    "            \n",
    "            # XGBoost Early Stopping\n",
    "            if 'XGB' in model_name:\n",
    "                best_params = model.best_params_.copy()\n",
    "                best_params['n_estimators'] = 500\n",
    "                \n",
    "                final_model = xgb.XGBRegressor(\n",
    "                    **best_params,\n",
    "                    early_stopping_rounds=20,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "\n",
    "                \n",
    "                final_model.fit(\n",
    "                    X_train_scaled, y_train,\n",
    "                    eval_set=[(X_train_scaled, y_train), (X_valid_scaled, y_valid)],\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                if hasattr(final_model, 'best_iteration'):\n",
    "                    print(f\"  Early stopping at iteration: {final_model.best_iteration}\")\n",
    "                model = final_model\n",
    "            \n",
    "            elif 'HistGBR' in model_name:\n",
    "                if hasattr(model.best_estimator_, 'n_iter_'):\n",
    "                    print(f\"  Iterations: {model.best_estimator_.n_iter_}\")\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluation\n",
    "        out = {}\n",
    "        pred_train = model.predict(X_train_scaled)\n",
    "        pred_valid = model.predict(X_valid_scaled)\n",
    "        pred_test = model.predict(X_test_scaled)\n",
    "        \n",
    "        out[\"train\"] = compute_metrics(y_train.to_numpy(), pred_train)\n",
    "        out[\"valid\"] = compute_metrics(y_valid.to_numpy(), pred_valid)\n",
    "        out[\"test\"] = compute_metrics(y_test.to_numpy(), pred_test)\n",
    "        \n",
    "        # Learning curve ì‹œê°í™”\n",
    "        plot_learning_curve(model, model_name, mode)\n",
    "        \n",
    "        return out, scaler, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad409ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8) Pipeline Runner\n",
    "# =========================\n",
    "def run_improved_pipeline_v2(\n",
    "    dfs,\n",
    "    mode,\n",
    "    time_col_map=None,\n",
    "    tz=None,\n",
    "    resample_rule=\"1h\",\n",
    "    resample_agg=\"mean\",\n",
    "    feature_cfg=FeatureConfig(),\n",
    "    split_cfg=SplitConfig(),\n",
    "    n_top_features=30,  # V2: ë” ì ì€ í”¼ì²˜\n",
    "    cv_splits=3,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ V2 (ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš© - ë³´ê°„ ì™„ë£Œ)\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Mode: {mode.upper()} (V2)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    target_cols = get_target_cols(mode)\n",
    "    \n",
    "    # 1) ë°ì´í„° ì¸ë±ì‹±\n",
    "    dfs_indexed = {}\n",
    "    for name, df in dfs.items():\n",
    "        if df is None or len(df) == 0:\n",
    "            dfs_indexed[name] = df\n",
    "            continue\n",
    "        \n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            dfs_indexed[name] = df.sort_index()\n",
    "        else:\n",
    "            if time_col_map is None or name not in time_col_map:\n",
    "                raise ValueError(f\"{name} has no DatetimeIndex and no time_col_map provided.\")\n",
    "            dfs_indexed[name] = set_datetime_index(df, time_col=time_col_map[name], tz=tz)\n",
    "    \n",
    "    # 2) ë³‘í•© (ì „ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” ì´ë¯¸ ë³‘í•©ë˜ì–´ ìˆìŒ)\n",
    "    df_all = merge_sources_on_time(dfs_indexed, how=\"outer\")\n",
    "    \n",
    "    # 3) ë¦¬ìƒ˜í”Œë§ (ì „ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” ì´ë¯¸ 1ë¶„ ê°„ê²©ì´ë¯€ë¡œ ì‹œê°„ ë‹¨ìœ„ë¡œ ë¦¬ìƒ˜í”Œë§)\n",
    "    df_hourly = resample_hourly(df_all, rule=resample_rule, agg=resample_agg)\n",
    "    \n",
    "    # 4) í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (V2: ë„ë©”ì¸ í”¼ì²˜ í¬í•¨)\n",
    "    df_feat = build_features(\n",
    "        df_hourly=df_hourly,\n",
    "        target_cols=target_cols,\n",
    "        feature_base_cols=None,\n",
    "        cfg=feature_cfg\n",
    "    )\n",
    "    \n",
    "    # 5) ì§€ë„í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± (ì „ì²˜ë¦¬ëœ ë°ì´í„°ì´ë¯€ë¡œ ê²°ì¸¡ì¹˜ ìµœì†Œ)\n",
    "    X, y = make_supervised_dataset(df_feat, target_cols=target_cols)\n",
    "    \n",
    "    # 6) ë¶„í• \n",
    "    splits = time_split(X, y, cfg=split_cfg)\n",
    "    X_train, y_train = splits[\"train\"]\n",
    "    \n",
    "    # 7) í”¼ì²˜ ì„ íƒ (V2: ë” ì ì€ í”¼ì²˜)\n",
    "    top_features = select_top_features(X_train, y_train, n_features=n_top_features)\n",
    "    X_train_selected = X_train[top_features]\n",
    "    X_valid_selected = splits[\"valid\"][0][top_features]\n",
    "    X_test_selected = splits[\"test\"][0][top_features]\n",
    "    \n",
    "    splits_selected = {\n",
    "        \"train\": (X_train_selected, y_train),\n",
    "        \"valid\": (X_valid_selected, splits[\"valid\"][1]),\n",
    "        \"test\": (X_test_selected, splits[\"test\"][1])\n",
    "    }\n",
    "    \n",
    "    # 8) ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (V2: ê°•í™”ëœ ì •ê·œí™”)\n",
    "    zoo = build_model_zoo_with_gridsearch(n_targets=y_train.shape[1], cv=cv_splits)\n",
    "    \n",
    "    results = {}\n",
    "    fitted_models = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    for model_name, base_model in zoo.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training: {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        res, scaler, fitted_model = fit_and_evaluate(\n",
    "            base_model, splits_selected, \n",
    "            model_name=model_name, \n",
    "            mode=mode,\n",
    "            target_cols=target_cols\n",
    "        )\n",
    "        \n",
    "        results[model_name] = res\n",
    "        fitted_models[model_name] = fitted_model\n",
    "        scalers[model_name] = scaler\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"\\n  Train - RÂ²: {res['train']['R2_mean']:.4f}, RMSE: {res['train']['RMSE_mean']:.2f}\")\n",
    "        print(f\"  Valid - RÂ²: {res['valid']['R2_mean']:.4f}, RMSE: {res['valid']['RMSE_mean']:.2f}\")\n",
    "        print(f\"  Test  - RÂ²: {res['test']['R2_mean']:.4f}, RMSE: {res['test']['RMSE_mean']:.2f}\")\n",
    "        \n",
    "        # íƒ€ê²Ÿë³„ ì„±ëŠ¥ ì¶œë ¥\n",
    "        if len(target_cols) > 1:\n",
    "            print(f\"\\n  Per-target Test RÂ²:\")\n",
    "            for i, target_name in enumerate(target_cols):\n",
    "                print(f\"    {target_name}: {res['test']['R2_by_target'][i]:.4f}\")\n",
    "        \n",
    "        # Overfitting ì²´í¬\n",
    "        train_r2 = res['train']['R2_mean']\n",
    "        valid_r2 = res['valid']['R2_mean']\n",
    "        if train_r2 - valid_r2 > 0.1:\n",
    "            print(f\"  âš ï¸  ê³¼ì í•© ê°€ëŠ¥ì„±: Train RÂ² ({train_r2:.4f}) >> Valid RÂ² ({valid_r2:.4f})\")\n",
    "\n",
    "    \n",
    "    metric_table = plot_metric_table(results, split=\"test\")\n",
    "    \n",
    "    # RÂ² ë¹„êµ ì‹œê°í™”\n",
    "    plot_r2_comparison(results, mode)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ìµœì¢… ê²°ê³¼ (Test Set)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(metric_table.to_string(index=False))\n",
    "    \n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"target_cols\": target_cols,\n",
    "        \"df_hourly\": df_hourly,\n",
    "        \"df_features\": df_feat,\n",
    "        \"X\": X, \"y\": y,\n",
    "        \"splits\": splits_selected,\n",
    "        \"top_features\": top_features,\n",
    "        \"results\": results,\n",
    "        \"metric_table\": metric_table,\n",
    "        \"fitted_models\": fitted_models,\n",
    "        \"scalers\": scalers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1067dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 9) Main Execution\n",
    "# =========================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IMPROVED ML BASELINE V2 - WWTP PREDICTION\")\n",
    "print(\"ì£¼ìš” ê°œì„ ì‚¬í•­:\")\n",
    "print(\"  1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš© (1ë¶„ ê°„ê²©, ì„ í˜• ë³´ê°„ ì™„ë£Œ)\")\n",
    "print(\"  2. ë„ë©”ì¸ í”¼ì²˜ ì¶”ê°€ (ìƒí˜¸ì‘ìš©, ë¹„ìœ¨, ì°¨ë¶„)\")\n",
    "print(\"  3. ì •ê·œí™” ê°•í™” (alpha ì¦ê°€, max_depth ì œí•œ)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\në°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ (1ë¶„ ê°„ê²©, ì„ í˜• ë³´ê°„ ì™„ë£Œ)\n",
    "df_flow = pd.read_csv(\"../../../../data/processed/flow_proc.csv\")\n",
    "df_tms = pd.read_csv(\"../../../../data/processed/tms_proc.csv\")\n",
    "df_all = pd.read_csv(\"../../../../data/processed/all_proc.csv\")\n",
    "\n",
    "print(f\"FLOW processed shape: {df_flow.shape}\")\n",
    "print(f\"TMS processed shape: {df_tms.shape}\")\n",
    "print(f\"ALL processed shape: {df_all.shape}\")\n",
    "\n",
    "# Q_in ì»¬ëŸ¼ ìƒì„±: Q_in = flow_TankA + flow_TankB\n",
    "if 'Q_in' not in df_flow.columns:\n",
    "    if 'flow_TankA' in df_flow.columns and 'flow_TankB' in df_flow.columns:\n",
    "        df_flow['Q_in'] = df_flow['flow_TankA'] + df_flow['flow_TankB']\n",
    "        print(\"âœ“ Q_in ì»¬ëŸ¼ ìƒì„± (flow): flow_TankA + flow_TankB\")\n",
    "\n",
    "if 'Q_in' not in df_all.columns:\n",
    "    if 'flow_TankA' in df_all.columns and 'flow_TankB' in df_all.columns:\n",
    "        df_all['Q_in'] = df_all['flow_TankA'] + df_all['flow_TankB']\n",
    "        print(\"âœ“ Q_in ì»¬ëŸ¼ ìƒì„± (all): flow_TankA + flow_TankB\")\n",
    "\n",
    "# ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€\n",
    "for c in [\"flow_TankA\", \"flow_TankB\"]:\n",
    "    if c in df_flow.columns:\n",
    "        df_flow.drop(columns=[c], inplace=True)\n",
    "        print(f\"âœ“ Dropped {c} from flow to prevent data leakage\")\n",
    "    if c in df_all.columns:\n",
    "        df_all.drop(columns=[c], inplace=True)\n",
    "        print(f\"âœ“ Dropped {c} from all to prevent data leakage\")\n",
    "\n",
    "dfs = {\n",
    "    \"flow\": df_flow,\n",
    "    \"tms\": df_tms,\n",
    "    \"all\": df_all\n",
    "}\n",
    "\n",
    "time_col_map = {\n",
    "    \"flow\": \"SYS_TIME\",\n",
    "    \"tms\": \"SYS_TIME\",\n",
    "    \"all\": \"SYS_TIME\"\n",
    "}\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Flow ì˜ˆì¸¡ (Q_inë§Œ ì˜ˆì¸¡)\n",
    "print(\"\\n\\n\" + \"ğŸ”µ\"*40)\n",
    "print(\"FLOW ì˜ˆì¸¡ ì‹œì‘\")\n",
    "print(\"ğŸ”µ\"*40)\n",
    "out_flow = run_improved_pipeline_v2(\n",
    "    {\"flow\": df_flow},  # flow ë°ì´í„°ë§Œ ì „ë‹¬\n",
    "    mode=\"flow\", \n",
    "    time_col_map={\"flow\": \"SYS_TIME\"},\n",
    "    n_top_features=25,\n",
    "    cv_splits=3\n",
    ")\n",
    "\n",
    "# TMS ì˜ˆì¸¡ (6ê°œ ìˆ˜ì§ˆ ë³€ìˆ˜ ì˜ˆì¸¡)\n",
    "print(\"\\n\\n\" + \"ğŸŸ¢\"*40)\n",
    "print(\"TMS ì˜ˆì¸¡ ì‹œì‘\")\n",
    "print(\"ğŸŸ¢\"*40)\n",
    "out_tms = run_improved_pipeline_v2(\n",
    "    {\"tms\": df_tms},  # tms ë°ì´í„°ë§Œ ì „ë‹¬\n",
    "    mode=\"tms\", \n",
    "    time_col_map={\"tms\": \"SYS_TIME\"},\n",
    "    n_top_features=30,\n",
    "    cv_splits=3\n",
    ")\n",
    "\n",
    "# All ì˜ˆì¸¡ (Q_in + 6ê°œ ìˆ˜ì§ˆ ë³€ìˆ˜ = 7ê°œ ì˜ˆì¸¡)\n",
    "print(\"\\n\\n\" + \"ğŸŸ¡\"*40)\n",
    "print(\"ALL ì˜ˆì¸¡ ì‹œì‘\")\n",
    "print(\"ğŸŸ¡\"*40)\n",
    "out_all = run_improved_pipeline_v2(\n",
    "    {\"all\": df_all},  # all ë°ì´í„°ë§Œ ì „ë‹¬\n",
    "    mode=\"all\", \n",
    "    time_col_map={\"all\": \"SYS_TIME\"},\n",
    "    n_top_features=35,\n",
    "    cv_splits=3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ìµœê³  ëª¨ë¸ ì €ì¥\n",
    "best_model_name = out_all[\"metric_table\"].iloc[0][\"model\"]\n",
    "print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}\")\n",
    "print(f\"   Test RÂ²: {out_all['results'][best_model_name]['test']['R2_mean']:.4f}\")\n",
    "print(f\"   Test RMSE: {out_all['results'][best_model_name]['test']['RMSE_mean']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
