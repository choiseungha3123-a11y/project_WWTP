# src/ML ì‹¤í–‰ ìˆœì„œ ë° ëª¨ë“ˆ ì„¤ëª…

## ğŸ“‹ ëª©ì°¨
1. [ì „ì²´ ì‹¤í–‰ íë¦„](#ì „ì²´-ì‹¤í–‰-íë¦„)
2. [ëª¨ë“ˆë³„ ìƒì„¸ ì„¤ëª…](#ëª¨ë“ˆë³„-ìƒì„¸-ì„¤ëª…)
3. [íŒŒì´í”„ë¼ì¸ ì¢…ë¥˜](#íŒŒì´í”„ë¼ì¸-ì¢…ë¥˜)
4. [ì‹¤í–‰ ì˜ˆì‹œ](#ì‹¤í–‰-ì˜ˆì‹œ)

---

## ğŸ”„ ì „ì²´ ì‹¤í–‰ íë¦„

### ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ (`run_pipeline`)

```
1. io.py
   â”œâ”€ load_csvs()           # CSV íŒŒì¼ ë¡œë“œ
   â”œâ”€ prep_flow()           # FLOW ë°ì´í„° ì „ì²˜ë¦¬
   â”œâ”€ prep_aws()            # AWS ë°ì´í„° ì „ì²˜ë¦¬
   â””â”€ set_datetime_index()  # ì‹œê°„ ì¸ë±ìŠ¤ ì„¤ì •
   
2. io.py
   â””â”€ merge_sources_on_time()  # ë°ì´í„° ë³‘í•©
   
3. preprocess.py
   â”œâ”€ resample_hourly()         # ë¦¬ìƒ˜í”Œë§ (1ì‹œê°„ ë‹¨ìœ„)
   â”œâ”€ impute_missing()          # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
   â””â”€ detect_and_handle_outliers()  # ì´ìƒì¹˜ ì²˜ë¦¬
   
4. features.py
   â”œâ”€ build_features()          # í”¼ì²˜ ìƒì„±
   â”‚  â”œâ”€ add_time_features()    # ì‹œê°„ íŠ¹ì„±
   â”‚  â”œâ”€ add_lag_features()     # Lag íŠ¹ì„±
   â”‚  â”œâ”€ add_rolling_features() # Rolling í†µê³„
   â”‚  â””â”€ add_model_specific_features()  # ëª¨ë¸ë³„ ë„ë©”ì¸ íŠ¹ì„±
   â””â”€ make_supervised_dataset() # X, y ë¶„ë¦¬
   
5. split.py
   â””â”€ time_split()              # ì‹œê³„ì—´ ë°ì´í„° ë¶„í•  (train/valid/test)
   
6. models.py
   â””â”€ build_model_zoo()         # ì—¬ëŸ¬ ML ëª¨ë¸ ìƒì„±
      â”œâ”€ LinearRegression
      â”œâ”€ Ridge
      â”œâ”€ Lasso
      â”œâ”€ RandomForest
      â”œâ”€ GradientBoosting
      â”œâ”€ XGBoost
      â””â”€ HistGradientBoosting
   
7. metrics.py
   â”œâ”€ fit_and_evaluate()        # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
   â”œâ”€ compute_metrics()         # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
   â””â”€ plot_metric_table()       # ê²°ê³¼ ì‹œê°í™”
```

### ê°œì„  íŒŒì´í”„ë¼ì¸ (`run_improved_pipeline`)

```
1~4. [ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼]

5. feature_selection.py
   â””â”€ select_top_features()     # í”¼ì²˜ ì„ íƒ (ìƒìœ„ Nê°œ)
      â””â”€ SelectKBest (mutual_info_regression)
   
6. scaling.py
   â””â”€ scale_data()              # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
      â””â”€ StandardScaler
   
7. split.py
   â””â”€ time_split()              # ë°ì´í„° ë¶„í• 
   
8. models.py
   â””â”€ build_model_zoo_with_optuna()  # Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
      â”œâ”€ XGBoost (ìµœì í™”)
      â””â”€ HistGradientBoosting (ìµœì í™”)
   
9. metrics.py
   â”œâ”€ fit_and_evaluate()        # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
   â””â”€ compute_metrics()         # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
   
10. save_results.py
    â”œâ”€ save_metrics()           # ì§€í‘œ ì €ì¥
    â”œâ”€ save_predictions()       # ì˜ˆì¸¡ê°’ ì €ì¥
    â””â”€ save_model()             # ëª¨ë¸ ì €ì¥
    
11. visualization.py
    â”œâ”€ plot_learning_curve()    # í•™ìŠµ ê³¡ì„ 
    â””â”€ plot_r2_comparison()     # RÂ² ë¹„êµ ì°¨íŠ¸
```

### Sliding Window íŒŒì´í”„ë¼ì¸ (`run_sliding_window_pipeline`)

```
1~4. [ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼]

5. sliding_window.py
   â””â”€ create_sliding_windows()  # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
      # (window_size, horizon, stride)
   
6. feature_selection.py
   â””â”€ select_top_features()     # í”¼ì²˜ ì„ íƒ
   
7. scaling.py
   â””â”€ scale_data()              # ìŠ¤ì¼€ì¼ë§
   
8~11. [ê°œì„  íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼]
```

---

## ğŸ“¦ ëª¨ë“ˆë³„ ìƒì„¸ ì„¤ëª…

### 1. **io.py** - ë°ì´í„° ì…ì¶œë ¥
**ì—­í• **: CSV íŒŒì¼ ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬

**ì£¼ìš” í•¨ìˆ˜**:
- `load_csvs(data_root)`: ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ CSV íŒŒì¼ ë¡œë“œ
- `prep_flow(df)`: FLOW ë°ì´í„° ì „ì²˜ë¦¬ (Q_in ìƒì„±)
- `prep_aws(df1, df2, df3)`: AWS ë°ì´í„° ë³‘í•© ë° ì „ì²˜ë¦¬
- `set_datetime_index(df, time_col)`: ì‹œê°„ ì¸ë±ìŠ¤ ì„¤ì •
- `merge_sources_on_time(dfs, how)`: ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ ë³‘í•©

**ì…ë ¥**: CSV íŒŒì¼ ê²½ë¡œ
**ì¶œë ¥**: DataFrame ë”•ì…”ë„ˆë¦¬

---

### 2. **preprocess.py** - ë°ì´í„° ì „ì²˜ë¦¬
**ì—­í• **: ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ì²˜ë¦¬ ë° ë¦¬ìƒ˜í”Œë§

**ì£¼ìš” í•¨ìˆ˜**:
- `resample_hourly(df, rule, agg)`: ì‹œê°„ ë‹¨ìœ„ ë¦¬ìƒ˜í”Œë§
- `impute_missing_with_strategy(df, config)`: ê²°ì¸¡ì¹˜ ì²˜ë¦¬
  - ë‹¨ê¸°: Forward Fill
  - ì¤‘ê¸°: EWMA
  - ì¥ê¸°: Rolling Median ë˜ëŠ” NaN ìœ ì§€
- `detect_and_handle_outliers(df, config)`: ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬
  - ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜
  - í†µê³„ ê¸°ë°˜ (IQR, Z-score)

**ì…ë ¥**: ë³‘í•©ëœ DataFrame
**ì¶œë ¥**: ì „ì²˜ë¦¬ëœ DataFrame

---

### 3. **features.py** - í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
**ì—­í• **: ì‹œê°„, Lag, Rolling, ë„ë©”ì¸ íŠ¹í™” í”¼ì²˜ ìƒì„±

**ì£¼ìš” í•¨ìˆ˜**:
- `build_features(df, target_cols, mode, cfg)`: ì „ì²´ í”¼ì²˜ ìƒì„± íŒŒì´í”„ë¼ì¸
- `add_time_features(df)`: ì‹œê°„ íŠ¹ì„± (hour, dayofweek, season ë“±)
- `add_lag_features(df, cols, lags)`: Lag íŠ¹ì„± (1, 2, 3, 6, 12, 24ì‹œê°„)
- `add_rolling_features(df, cols, windows)`: Rolling í†µê³„ (mean, std, min, max)
- `add_model_specific_features(df, mode)`: ëª¨ë¸ë³„ ë„ë©”ì¸ íŠ¹ì„±
  - **FLOW**: ìˆ˜ìœ„-ìœ ëŸ‰, ê°•ìš° ê³µê°„ í†µí•©
  - **ModelA/B/C**: TMS ìƒí˜¸ì‘ìš©, ê°•ìˆ˜-TMS ìƒí˜¸ì‘ìš©
- `make_supervised_dataset(df, target_cols)`: X, y ë¶„ë¦¬

**ì…ë ¥**: ì „ì²˜ë¦¬ëœ DataFrame
**ì¶œë ¥**: í”¼ì²˜ê°€ ì¶”ê°€ëœ DataFrame, X, y

---

### 4. **split.py** - ë°ì´í„° ë¶„í• 
**ì—­í• **: ì‹œê³„ì—´ ë°ì´í„°ë¥¼ train/valid/testë¡œ ë¶„í• 

**ì£¼ìš” í•¨ìˆ˜**:
- `time_split(X, y, config)`: ì‹œê°„ ìˆœì„œ ìœ ì§€í•˜ë©° ë¶„í• 
  - Train: 70%
  - Valid: 15%
  - Test: 15%

**ì…ë ¥**: X, y
**ì¶œë ¥**: (X_train, y_train), (X_valid, y_valid), (X_test, y_test)

---

### 5. **feature_selection.py** - í”¼ì²˜ ì„ íƒ
**ì—­í• **: ì¤‘ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ (ì°¨ì› ì¶•ì†Œ)

**ì£¼ìš” í•¨ìˆ˜**:
- `select_top_features(X_train, y_train, n_features)`: ìƒìœ„ Nê°œ í”¼ì²˜ ì„ íƒ
  - SelectKBest + mutual_info_regression

**ì…ë ¥**: X_train, y_train, n_features
**ì¶œë ¥**: ì„ íƒëœ í”¼ì²˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸

---

### 6. **scaling.py** - ë°ì´í„° ìŠ¤ì¼€ì¼ë§
**ì—­í• **: í”¼ì²˜ ìŠ¤ì¼€ì¼ ì •ê·œí™”

**ì£¼ìš” í•¨ìˆ˜**:
- `scale_data(X_train, X_valid, X_test)`: StandardScaler ì ìš©

**ì…ë ¥**: ë¶„í• ëœ ë°ì´í„°
**ì¶œë ¥**: ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° + Scaler ê°ì²´

---

### 7. **models.py** - ëª¨ë¸ ìƒì„±
**ì—­í• **: ML ëª¨ë¸ ìƒì„± ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

**ì£¼ìš” í•¨ìˆ˜**:
- `build_model_zoo()`: ê¸°ë³¸ ëª¨ë¸ ì„¸íŠ¸ ìƒì„±
  - LinearRegression, Ridge, Lasso
  - RandomForest, GradientBoosting
  - XGBoost, HistGradientBoosting
  
- `build_model_zoo_with_optuna()`: Optunaë¡œ ìµœì í™”ëœ ëª¨ë¸ ìƒì„±
  - XGBoost (íŠœë‹)
  - HistGradientBoosting (íŠœë‹)

**ì…ë ¥**: í•™ìŠµ ë°ì´í„°, ì„¤ì •
**ì¶œë ¥**: ëª¨ë¸ ë”•ì…”ë„ˆë¦¬

---

### 8. **metrics.py** - ëª¨ë¸ í‰ê°€
**ì—­í• **: ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€

**ì£¼ìš” í•¨ìˆ˜**:
- `fit_and_evaluate(models, splits)`: ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
- `compute_metrics(y_true, y_pred)`: ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
  - MSE, RMSE, MAE, RÂ², MAPE
- `plot_metric_table(metric_table)`: ê²°ê³¼ í…Œì´ë¸” ì‹œê°í™”

**ì…ë ¥**: ëª¨ë¸, ë°ì´í„° ë¶„í• 
**ì¶œë ¥**: ì„±ëŠ¥ ì§€í‘œ í…Œì´ë¸”, í•™ìŠµëœ ëª¨ë¸

---

### 9. **sliding_window.py** - ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
**ì—­í• **: ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ ìœˆë„ìš° ìƒì„±

**ì£¼ìš” í•¨ìˆ˜**:
- `create_sliding_windows(X, y, window_size, horizon, stride)`: ìœˆë„ìš° ìƒì„±
  - window_size: ê³¼ê±° ëª‡ ì‹œê°„ ë³¼ ê²ƒì¸ì§€
  - horizon: ë¯¸ë˜ ëª‡ ì‹œê°„ í›„ ì˜ˆì¸¡
  - stride: ìœˆë„ìš° ì´ë™ ê°„ê²©

**ì…ë ¥**: X, y, ìœˆë„ìš° ì„¤ì •
**ì¶œë ¥**: X_seq (3D), y_seq (2D)

---

### 10. **save_results.py** - ê²°ê³¼ ì €ì¥
**ì—­í• **: ëª¨ë¸, ì˜ˆì¸¡ê°’, ì§€í‘œ ì €ì¥

**ì£¼ìš” í•¨ìˆ˜**:
- `save_metrics(metrics, path)`: ì„±ëŠ¥ ì§€í‘œ ì €ì¥ (JSON)
- `save_predictions(y_true, y_pred, path)`: ì˜ˆì¸¡ê°’ ì €ì¥ (CSV)
- `save_model(model, path)`: ëª¨ë¸ ì €ì¥ (pickle)

**ì…ë ¥**: ê²°ê³¼ ë°ì´í„°, ì €ì¥ ê²½ë¡œ
**ì¶œë ¥**: íŒŒì¼ ì €ì¥

---

### 11. **visualization.py** - ì‹œê°í™”
**ì—­í• **: í•™ìŠµ ê²°ê³¼ ì‹œê°í™”

**ì£¼ìš” í•¨ìˆ˜**:
- `plot_learning_curve(model, X, y)`: í•™ìŠµ ê³¡ì„ 
- `plot_r2_comparison(metric_table)`: RÂ² ë¹„êµ ì°¨íŠ¸
- `plot_predictions(y_true, y_pred)`: ì˜ˆì¸¡ vs ì‹¤ì œ ë¹„êµ

**ì…ë ¥**: ëª¨ë¸, ë°ì´í„°, ì§€í‘œ
**ì¶œë ¥**: ê·¸ë˜í”„ (matplotlib)

---

## ğŸš€ íŒŒì´í”„ë¼ì¸ ì¢…ë¥˜

### 1. ê¸°ë³¸ íŒŒì´í”„ë¼ì¸
**íŠ¹ì§•**: ì—¬ëŸ¬ ML ëª¨ë¸ ë¹„êµ, ë¹ ë¥¸ ì‹¤í–‰
**ì‚¬ìš© ì‹œê¸°**: ì´ˆê¸° íƒìƒ‰, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

```python
from src.ML.pipeline import run_pipeline

out = run_pipeline(
    dfs,
    mode="flow",
    time_col_map={"flow": "SYS_TIME", "tms": "SYS_TIME", "aws": "datetime"},
    resample_rule="1h",
    resample_agg="mean",
    random_state=42
)
```

### 2. ê°œì„  íŒŒì´í”„ë¼ì¸
**íŠ¹ì§•**: Optuna íŠœë‹, í”¼ì²˜ ì„ íƒ, ìŠ¤ì¼€ì¼ë§
**ì‚¬ìš© ì‹œê¸°**: ìµœì¢… ëª¨ë¸ í•™ìŠµ, ì„±ëŠ¥ ìµœì í™”

```python
from src.ML.pipeline import run_improved_pipeline

out = run_improved_pipeline(
    dfs,
    mode="flow",
    time_col_map={"flow": "SYS_TIME", "tms": "SYS_TIME", "aws": "datetime"},
    resample_rule="1h",
    n_top_features=50,
    cv_splits=3,
    n_trials=50,
    save_dir="results/ML"
)
```

### 3. Sliding Window íŒŒì´í”„ë¼ì¸
**íŠ¹ì§•**: ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ, LSTMê³¼ ìœ ì‚¬í•œ ì…ë ¥ êµ¬ì¡°
**ì‚¬ìš© ì‹œê¸°**: ì‹œê°„ì  ì˜ì¡´ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°

```python
from src.ML.pipeline import run_sliding_window_pipeline

out = run_sliding_window_pipeline(
    dfs,
    mode="flow",
    window_size=24,  # 24ì‹œê°„ ê³¼ê±° ë°ì´í„°
    horizon=1,       # 1ì‹œê°„ í›„ ì˜ˆì¸¡
    stride=1,        # ë§¤ ì‹œê°„ë§ˆë‹¤
    n_top_features=50,
    save_dir="results/ML"
)
```

---

## ğŸ’¡ ì‹¤í–‰ ì˜ˆì‹œ

### CLI ì‹¤í–‰
```bash
# ê¸°ë³¸ íŒŒì´í”„ë¼ì¸
python scripts/ML/train.py --mode flow --data-root data/actual --resample 1h

# ê°œì„  íŒŒì´í”„ë¼ì¸
python scripts/ML/train.py --mode flow --improved --n-features 50 --n-trials 50

# Sliding Window íŒŒì´í”„ë¼ì¸
python scripts/ML/train.py --mode flow --sliding-window --window-size 24 --horizon 1
```

### ë…¸íŠ¸ë¶ ì‹¤í–‰
```python
# notebook/ML/train_ml_models.ipynb ì°¸ê³ 
PIPELINE_TYPE = "improved"  # "basic", "improved", "sliding_window"
MODE = "flow"
RESAMPLE = "1h"
```

---

## ğŸ“Š ëª¨ë“œë³„ íƒ€ê²Ÿ ë° ì…ë ¥ ë°ì´í„°

| ëª¨ë“œ | íƒ€ê²Ÿ | ì…ë ¥ ë°ì´í„° |
|------|------|-------------|
| **flow** | Q_in | FLOW (level) + AWS (ê°•ìˆ˜, ê¸°ìƒ) |
| **tms** | ì „ì²´ 6ê°œ TMS | FLOW + AWS + TMS |
| **modelA** | TOC_VU, SS_VU | FLOW + AWS + TMS (PH, FLUX, TN, TP) |
| **modelB** | TN_VU, TP_VU | FLOW + AWS + TMS (PH, FLUX, TOC, SS) |
| **modelC** | FLUX_VU, PH_VU | FLOW + AWS + TMS (TOC, SS, TN, TP) |

---

## ğŸ” ë””ë²„ê¹… íŒ

1. **ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨**: `io.py`ì˜ `load_csvs()` í™•ì¸
2. **ê²°ì¸¡ì¹˜ ë§ìŒ**: `preprocess.py`ì˜ `impute_missing()` ì„¤ì • ì¡°ì •
3. **ì„±ëŠ¥ ë‚®ìŒ**: `features.py`ì˜ ë„ë©”ì¸ íŠ¹ì„± ì¶”ê°€ ë˜ëŠ” `feature_selection.py`ë¡œ í”¼ì²˜ ì„ íƒ
4. **ê³¼ì í•©**: `models.py`ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • ë˜ëŠ” ì •ê·œí™” ê°•í™”
5. **í•™ìŠµ ëŠë¦¼**: ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ë˜ëŠ” `n_trials` ê°ì†Œ

---

## ğŸ“ ì°¸ê³  ìë£Œ

- ê° ëª¨ë“ˆì˜ docstring ì°¸ê³ 
- `scripts/ML/train.py`: CLI ì‹¤í–‰ ì˜ˆì‹œ
- `notebook/ML/train_ml_models.ipynb`: ë…¸íŠ¸ë¶ ì‹¤í–‰ ì˜ˆì‹œ
