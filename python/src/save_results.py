"""
ê²°ê³¼ ì €ì¥ ëª¨ë“ˆ
ì˜ˆì¸¡ê°’, ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹, ëª¨ë¸ ë“±ì„ ì €ì¥
"""

import os
import pickle
from pathlib import Path
from typing import Dict, Any, Optional, Union
import numpy as np
import pandas as pd
from datetime import datetime


def save_predictions(
    y_true: Union[pd.DataFrame, np.ndarray],
    y_pred: np.ndarray,
    split_name: str,
    target_cols: list,
    save_dir: str,
    index: Optional[pd.Index] = None
) -> str:
    """
    ì˜ˆì¸¡ê°’ì„ CSV íŒŒì¼ë¡œ ì €ì¥
    
    Parameters:
    -----------
    y_true : DataFrame or ndarray
        ì‹¤ì œ ê°’
    y_pred : ndarray
        ì˜ˆì¸¡ ê°’
    split_name : str
        ë°ì´í„° ë¶„í•  ì´ë¦„ (train/valid/test)
    target_cols : list
        íƒ€ê²Ÿ ì»¬ëŸ¼ ì´ë¦„
    save_dir : str
        ì €ì¥ ë””ë ‰í† ë¦¬
    index : Index, optional
        ì‹œê°„ ì¸ë±ìŠ¤
        
    Returns:
    --------
    str : ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # DataFrame ìƒì„±
    if isinstance(y_true, pd.DataFrame):
        df_true = y_true.copy()
        if index is not None:
            df_true.index = index
    else:
        df_true = pd.DataFrame(y_true, columns=target_cols, index=index)
    
    df_pred = pd.DataFrame(y_pred, columns=[f"{col}_pred" for col in target_cols], index=df_true.index)
    
    # ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ë³‘í•©
    df_result = pd.concat([df_true, df_pred], axis=1)
    
    # ì˜¤ì°¨ ê³„ì‚°
    for col in target_cols:
        df_result[f"{col}_error"] = df_result[col] - df_result[f"{col}_pred"]
        df_result[f"{col}_error_pct"] = (df_result[f"{col}_error"] / df_result[col]) * 100
    
    # ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"predictions_{split_name}_{timestamp}.csv"
    filepath = os.path.join(save_dir, filename)
    df_result.to_csv(filepath)
    
    print(f"  ğŸ’¾ ì˜ˆì¸¡ê°’ ì €ì¥: {filepath}")
    return filepath


def save_sequence_dataset(
    X_seq: np.ndarray,
    y_seq: np.ndarray,
    split_name: str,
    feature_names: list,
    target_cols: list,
    window_size: int,
    save_dir: str,
    save_format: str = "npz"
) -> str:
    """
    ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ì„ ì €ì¥
    
    Parameters:
    -----------
    X_seq : ndarray
        ì…ë ¥ ì‹œí€€ìŠ¤ (samples, window_size, features) ë˜ëŠ” (samples, features)
    y_seq : ndarray
        íƒ€ê²Ÿ ì‹œí€€ìŠ¤ (samples, targets)
    split_name : str
        ë°ì´í„° ë¶„í•  ì´ë¦„ (train/valid/test)
    feature_names : list
        íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    target_cols : list
        íƒ€ê²Ÿ ì»¬ëŸ¼ ì´ë¦„
    window_size : int
        ìœˆë„ìš° í¬ê¸°
    save_dir : str
        ì €ì¥ ë””ë ‰í† ë¦¬
    save_format : str
        ì €ì¥ í˜•ì‹ ('npz', 'pickle', 'csv')
        
    Returns:
    --------
    str : ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    os.makedirs(save_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if save_format == "npz":
        # NumPy ì••ì¶• í˜•ì‹ (ê¶Œì¥ - ë¹ ë¥´ê³  ìš©ëŸ‰ ì‘ìŒ)
        filename = f"sequence_{split_name}_{timestamp}.npz"
        filepath = os.path.join(save_dir, filename)
        
        np.savez_compressed(
            filepath,
            X=X_seq,
            y=y_seq,
            feature_names=feature_names,
            target_cols=target_cols,
            window_size=window_size,
            split_name=split_name
        )
        
    elif save_format == "pickle":
        # Pickle í˜•ì‹
        filename = f"sequence_{split_name}_{timestamp}.pkl"
        filepath = os.path.join(save_dir, filename)
        
        data = {
            'X': X_seq,
            'y': y_seq,
            'feature_names': feature_names,
            'target_cols': target_cols,
            'window_size': window_size,
            'split_name': split_name
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
    
    elif save_format == "csv":
        # CSV í˜•ì‹ (ì‚¬ëŒì´ ì½ê¸° ì‰¬ì›€, ìš©ëŸ‰ í¼)
        filename = f"sequence_{split_name}_{timestamp}.csv"
        filepath = os.path.join(save_dir, filename)
        
        # 3Dë¥¼ 2Dë¡œ í‰íƒ„í™”
        if X_seq.ndim == 3:
            n_samples, window_size, n_features = X_seq.shape
            X_flat = X_seq.reshape(n_samples, -1)
            
            # íŠ¹ì„± ì´ë¦„ ìƒì„±
            flat_feature_names = []
            for t in range(window_size - 1, -1, -1):
                time_label = f"t-{t}" if t > 0 else "t0"
                for feat in feature_names:
                    flat_feature_names.append(f"{feat}_{time_label}")
        else:
            X_flat = X_seq
            flat_feature_names = feature_names
        
        # DataFrame ìƒì„±
        df_X = pd.DataFrame(X_flat, columns=flat_feature_names)
        df_y = pd.DataFrame(y_seq, columns=target_cols)
        df_result = pd.concat([df_X, df_y], axis=1)
        
        df_result.to_csv(filepath, index=False)
    
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {save_format}. 'npz', 'pickle', 'csv' ì¤‘ ì„ íƒí•˜ì„¸ìš”.")
    
    print(f"  ğŸ’¾ ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥: {filepath}")
    print(f"     - í˜•ì‹: {save_format}")
    print(f"     - X shape: {X_seq.shape}")
    print(f"     - y shape: {y_seq.shape}")
    
    return filepath


def load_sequence_dataset(filepath: str) -> Dict[str, Any]:
    """
    ì €ì¥ëœ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ë¡œë“œ
    
    Parameters:
    -----------
    filepath : str
        íŒŒì¼ ê²½ë¡œ
        
    Returns:
    --------
    dict : ë¡œë“œëœ ë°ì´í„°
    """
    ext = Path(filepath).suffix
    
    if ext == ".npz":
        data = np.load(filepath, allow_pickle=True)
        return {
            'X': data['X'],
            'y': data['y'],
            'feature_names': data['feature_names'].tolist(),
            'target_cols': data['target_cols'].tolist(),
            'window_size': int(data['window_size']),
            'split_name': str(data['split_name'])
        }
    
    elif ext == ".pkl":
        with open(filepath, 'rb') as f:
            return pickle.load(f)
    
    elif ext == ".csv":
        df = pd.read_csv(filepath)
        # CSVëŠ” ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ìˆ˜ë™ìœ¼ë¡œ ë¶„ë¦¬ í•„ìš”
        print("âš ï¸  CSV í˜•ì‹ì€ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Xì™€ yë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.")
        return {'data': df}
    
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")


def save_model_and_metadata(
    model: Any,
    scaler: Any,
    top_features: list,
    metadata: Dict[str, Any],
    save_dir: str,
    model_name: str = "best_model"
) -> Dict[str, str]:
    """
    ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ë©”íƒ€ë°ì´í„° ì €ì¥
    
    Parameters:
    -----------
    model : Any
        í•™ìŠµëœ ëª¨ë¸
    scaler : Any
        ìŠ¤ì¼€ì¼ëŸ¬ (StandardScaler ë“±)
    top_features : list
        ì„ íƒëœ íŠ¹ì„± ë¦¬ìŠ¤íŠ¸
    metadata : dict
        ë©”íƒ€ë°ì´í„° (mode, window_size, horizon ë“±)
    save_dir : str
        ì €ì¥ ë””ë ‰í† ë¦¬
    model_name : str
        ëª¨ë¸ ì´ë¦„
        
    Returns:
    --------
    dict : ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    os.makedirs(save_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    filepaths = {}
    
    # ëª¨ë¸ ì €ì¥
    model_path = os.path.join(save_dir, f"{model_name}_{timestamp}.pkl")
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    filepaths['model'] = model_path
    print(f"  ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    if scaler is not None:
        scaler_path = os.path.join(save_dir, f"scaler_{timestamp}.pkl")
        with open(scaler_path, 'wb') as f:
            pickle.dump(scaler, f)
        filepaths['scaler'] = scaler_path
        print(f"  ğŸ’¾ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")
    
    # íŠ¹ì„± ë¦¬ìŠ¤íŠ¸ ì €ì¥
    if top_features is not None:
        features_path = os.path.join(save_dir, f"features_{timestamp}.txt")
        with open(features_path, 'w') as f:
            f.write('\n'.join(top_features))
        filepaths['features'] = features_path
        print(f"  ğŸ’¾ íŠ¹ì„± ë¦¬ìŠ¤íŠ¸ ì €ì¥: {features_path}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_path = os.path.join(save_dir, f"metadata_{timestamp}.pkl")
    with open(metadata_path, 'wb') as f:
        pickle.dump(metadata, f)
    filepaths['metadata'] = metadata_path
    print(f"  ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
    
    return filepaths


def save_all_results(
    result: Dict[str, Any],
    save_dir: str,
    save_predictions_flag: bool = True,
    save_sequences_flag: bool = True,
    save_model_flag: bool = True,
    sequence_format: str = "npz"
) -> Dict[str, Any]:
    """
    íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ ëª¨ë‘ ì €ì¥
    
    Parameters:
    -----------
    result : dict
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼
    save_dir : str
        ì €ì¥ ë””ë ‰í† ë¦¬
    save_predictions_flag : bool
        ì˜ˆì¸¡ê°’ ì €ì¥ ì—¬ë¶€
    save_sequences_flag : bool
        ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥ ì—¬ë¶€
    save_model_flag : bool
        ëª¨ë¸ ì €ì¥ ì—¬ë¶€
    sequence_format : str
        ì‹œí€€ìŠ¤ ì €ì¥ í˜•ì‹ ('npz', 'pickle', 'csv')
        
    Returns:
    --------
    dict : ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    saved_files = {
        'predictions': {},
        'sequences': {},
        'models': {}
    }
    
    print(f"\n{'='*60}")
    print("ê²°ê³¼ ì €ì¥ ì¤‘...")
    print(f"{'='*60}")
    
    # 1. ì˜ˆì¸¡ê°’ ì €ì¥
    if save_predictions_flag and 'fitted_models' in result and result['fitted_models'] and 'splits' in result:
        print("\n[1/3] ì˜ˆì¸¡ê°’ ì €ì¥ ì¤‘...")
        
        best_model_name = result['metric_table'].iloc[0]['model']
        fitted_model = result['fitted_models'][best_model_name]
        
        # fitted_modelì´ dictì¸ ê²½ìš° (ë‹¤ì¤‘ íƒ€ê²Ÿ) - ê° íƒ€ê²Ÿë³„ë¡œ ì˜ˆì¸¡
        if isinstance(fitted_model, dict):
            print("  ë‹¤ì¤‘ íƒ€ê²Ÿ ëª¨ë¸ - íƒ€ê²Ÿë³„ ì˜ˆì¸¡ê°’ ì €ì¥...")
            for split_name, (X, y) in result['splits'].items():
                # ê° íƒ€ê²Ÿë³„ë¡œ ì˜ˆì¸¡
                y_pred_list = []
                for target_name in result['target_cols']:
                    model = fitted_model[target_name]
                    y_pred_single = model.predict(X)
                    y_pred_list.append(y_pred_single)
                
                # ë‹¤ì¤‘ íƒ€ê²Ÿ ì˜ˆì¸¡ê°’ ê²°í•©
                y_pred = np.column_stack(y_pred_list)
                
                filepath = save_predictions(
                    y_true=y,
                    y_pred=y_pred,
                    split_name=split_name,
                    target_cols=result['target_cols'],
                    save_dir=os.path.join(save_dir, 'predictions'),
                    index=y.index if hasattr(y, 'index') else None
                )
                saved_files['predictions'][split_name] = filepath
        else:
            # ë‹¨ì¼ íƒ€ê²Ÿ ë˜ëŠ” ì¼ë°˜ ëª¨ë¸
            for split_name, (X, y) in result['splits'].items():
                y_pred = fitted_model.predict(X)
                
                filepath = save_predictions(
                    y_true=y,
                    y_pred=y_pred,
                    split_name=split_name,
                    target_cols=result['target_cols'],
                    save_dir=os.path.join(save_dir, 'predictions'),
                    index=y.index if hasattr(y, 'index') else None
                )
                saved_files['predictions'][split_name] = filepath
    elif save_predictions_flag:
        print("\n[1/3] ì˜ˆì¸¡ê°’ ì €ì¥ ê±´ë„ˆë›°ê¸° (fitted_models ì—†ìŒ)")
    
    # 2. ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥ (Sliding Windowì¸ ê²½ìš°)
    if save_sequences_flag and 'X_seq' in result and 'y_seq' in result:
        print("\n[2/3] ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥ ì¤‘...")
        
        # ì›ë³¸ ì‹œí€€ìŠ¤ ì €ì¥ (ë¶„í•  ì „)
        filepath = save_sequence_dataset(
            X_seq=result['X_seq'],
            y_seq=result['y_seq'],
            split_name='all',
            feature_names=result['X_original'].columns.tolist() if hasattr(result['X_original'], 'columns') else [],
            target_cols=result['target_cols'],
            window_size=result.get('window_size', 24),
            save_dir=os.path.join(save_dir, 'sequences'),
            save_format=sequence_format
        )
        saved_files['sequences']['all'] = filepath
        
        # ë¶„í• ëœ ì‹œí€€ìŠ¤ ì €ì¥ (ì„ íƒì‚¬í•­)
        # Train/Valid/Test ê°ê° ì €ì¥í•˜ë ¤ë©´ ì—¬ê¸°ì— ì¶”ê°€
    
    # 3. ëª¨ë¸ ë° ë©”íƒ€ë°ì´í„° ì €ì¥
    if save_model_flag and 'fitted_models' in result and result['fitted_models']:
        print("\n[3/3] ëª¨ë¸ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")
        
        best_model_name = result['metric_table'].iloc[0]['model']
        fitted_model = result['fitted_models'][best_model_name]
        
        metadata = {
            'mode': result.get('mode'),
            'target_cols': result.get('target_cols'),
            'window_size': result.get('window_size'),
            'horizon': result.get('horizon'),
            'stride': result.get('stride'),
            'best_model_name': best_model_name,
            'test_r2': result['metric_table'].iloc[0]['R2_mean'],
            'test_rmse': result['metric_table'].iloc[0]['RMSE_mean'],
            'n_samples_original': len(result.get('X_original', [])),
            'n_windows': len(result.get('X_seq', [])) if 'X_seq' in result else None,
            'n_features_original': result['X_original'].shape[1] if 'X_original' in result else None,
            'n_features_selected': len(result.get('top_features', [])) if result.get('top_features') else None,
            'is_multi_target': isinstance(fitted_model, dict)
        }
        
        # ë‹¤ì¤‘ íƒ€ê²Ÿì¸ ê²½ìš° ì²« ë²ˆì§¸ íƒ€ê²Ÿ ëª¨ë¸ ì €ì¥ (ë˜ëŠ” ì „ì²´ dict ì €ì¥)
        if isinstance(fitted_model, dict):
            print(f"  ë‹¤ì¤‘ íƒ€ê²Ÿ ëª¨ë¸ - {len(fitted_model)}ê°œ íƒ€ê²Ÿë³„ ëª¨ë¸ ì €ì¥...")
            # ì „ì²´ dictë¥¼ ì €ì¥
            model_to_save = fitted_model
        else:
            model_to_save = fitted_model
        
        filepaths = save_model_and_metadata(
            model=model_to_save,
            scaler=result.get('scaler'),
            top_features=result.get('top_features'),
            metadata=metadata,
            save_dir=os.path.join(save_dir, 'models'),
            model_name=best_model_name
        )
        saved_files['models'] = filepaths
    elif save_model_flag:
        print("\n[3/3] ëª¨ë¸ ì €ì¥ ê±´ë„ˆë›°ê¸° (fitted_models ì—†ìŒ)")
    
    print(f"\n{'='*60}")
    print("ì €ì¥ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ì €ì¥ ìœ„ì¹˜: {save_dir}")
    
    return saved_files


# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

if __name__ == "__main__":
    """
    ê²°ê³¼ ì €ì¥ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
    """
    print("ê²°ê³¼ ì €ì¥ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸\n")
    
    # ì˜ˆì‹œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 100
    window_size = 24
    n_features = 5
    n_targets = 2
    
    # ì‹œí€€ìŠ¤ ë°ì´í„°
    X_seq = np.random.rand(n_samples, window_size, n_features)
    y_seq = np.random.rand(n_samples, n_targets)
    
    feature_names = [f'feature_{i}' for i in range(n_features)]
    target_cols = ['target_0', 'target_1']
    
    # 1. ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥ (NPZ)
    print("1. NPZ í˜•ì‹ìœ¼ë¡œ ì €ì¥:")
    filepath_npz = save_sequence_dataset(
        X_seq, y_seq, 'test', feature_names, target_cols, window_size,
        save_dir='test_results/sequences',
        save_format='npz'
    )
    
    # 2. ì‹œí€€ìŠ¤ ë°ì´í„° ë¡œë“œ
    print("\n2. NPZ íŒŒì¼ ë¡œë“œ:")
    loaded_data = load_sequence_dataset(filepath_npz)
    print(f"   ë¡œë“œëœ X shape: {loaded_data['X'].shape}")
    print(f"   ë¡œë“œëœ y shape: {loaded_data['y'].shape}")
    
    # 3. ì˜ˆì¸¡ê°’ ì €ì¥
    print("\n3. ì˜ˆì¸¡ê°’ ì €ì¥:")
    y_true = pd.DataFrame(y_seq, columns=target_cols)
    y_pred = y_seq + np.random.randn(*y_seq.shape) * 0.1
    
    filepath_pred = save_predictions(
        y_true, y_pred, 'test', target_cols,
        save_dir='test_results/predictions'
    )
    
    print("\nâœ… ê²°ê³¼ ì €ì¥ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"   í…ŒìŠ¤íŠ¸ íŒŒì¼ ìœ„ì¹˜: test_results/")
